% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/aiplatform_objects.R
\name{GoogleCloudAiplatformV1SchemaModelevaluationMetricsClassificationEvaluationMetricsConfidenceMetrics}
\alias{GoogleCloudAiplatformV1SchemaModelevaluationMetricsClassificationEvaluationMetricsConfidenceMetrics}
\title{GoogleCloudAiplatformV1SchemaModelevaluationMetricsClassificationEvaluationMetricsConfidenceMetrics Object}
\usage{
GoogleCloudAiplatformV1SchemaModelevaluationMetricsClassificationEvaluationMetricsConfidenceMetrics(
  recall = NULL,
  falsePositiveCount = NULL,
  f1ScoreMacro = NULL,
  trueNegativeCount = NULL,
  truePositiveCount = NULL,
  confidenceThreshold = NULL,
  confusionMatrix = NULL,
  f1Score = NULL,
  f1ScoreMicro = NULL,
  precisionAt1 = NULL,
  falseNegativeCount = NULL,
  falsePositiveRate = NULL,
  f1ScoreAt1 = NULL,
  recallAt1 = NULL,
  falsePositiveRateAt1 = NULL,
  precision = NULL,
  maxPredictions = NULL
)
}
\arguments{
\item{recall}{Recall (True Positive Rate) for the given confidence threshold}

\item{falsePositiveCount}{The number of Model created labels that do not match a ground truth label}

\item{f1ScoreMacro}{Macro-averaged F1 Score}

\item{trueNegativeCount}{The number of labels that were not created by the Model, but if they would, they would not match a ground truth label}

\item{truePositiveCount}{The number of Model created labels that match a ground truth label}

\item{confidenceThreshold}{Metrics are computed with an assumption that the Model never returns predictions with score lower than this value}

\item{confusionMatrix}{Confusion matrix of the evaluation for this confidence_threshold}

\item{f1Score}{The harmonic mean of recall and precision}

\item{f1ScoreMicro}{Micro-averaged F1 Score}

\item{precisionAt1}{The precision when only considering the label that has the highest prediction score and not below the confidence threshold for each DataItem}

\item{falseNegativeCount}{The number of ground truth labels that are not matched by a Model created label}

\item{falsePositiveRate}{False Positive Rate for the given confidence threshold}

\item{f1ScoreAt1}{The harmonic mean of recallAt1 and precisionAt1}

\item{recallAt1}{The Recall (True Positive Rate) when only considering the label that has the highest prediction score and not below the confidence threshold for each DataItem}

\item{falsePositiveRateAt1}{The False Positive Rate when only considering the label that has the highest prediction score and not below the confidence threshold for each DataItem}

\item{precision}{Precision for the given confidence threshold}

\item{maxPredictions}{Metrics are computed with an assumption that the Model always returns at most this many predictions (ordered by their score, descendingly), but they all still need to meet the \code{confidenceThreshold}}
}
\value{
GoogleCloudAiplatformV1SchemaModelevaluationMetricsClassificationEvaluationMetricsConfidenceMetrics object
}
\description{
GoogleCloudAiplatformV1SchemaModelevaluationMetricsClassificationEvaluationMetricsConfidenceMetrics Object
}
\details{
Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
No description
}
\concept{GoogleCloudAiplatformV1SchemaModelevaluationMetricsClassificationEvaluationMetricsConfidenceMetrics functions}
