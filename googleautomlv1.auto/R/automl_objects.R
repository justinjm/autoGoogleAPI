#' Cloud AutoML API Objects 
#' Train high-quality custom machine learning models with minimum effort and machine learning expertise.
#' 
#' Auto-generated code by googleAuthR::gar_create_api_objects
#'  at 2019-12-10 17:15:25
#' filename: /Users/justinmarciszewski/Dropbox/dev/R/autoGoogleAPI/googleautomlv1.auto/R/automl_objects.R
#' api_json: api_json
#' 
#' Objects for use by the functions created by googleAuthR::gar_create_api_skeleton

#' Layout Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Describes the layout information of a text_segment in the document.
#' 
#' @param boundingPoly The position of the text_segment in the page
#' @param textSegmentType The type of the text_segment in document
#' @param textSegment Text Segment that represents a segment in
#' @param pageNumber Page number of the text_segment in the original document, starts
#' 
#' @return Layout object
#' 
#' @family Layout functions
#' @export
Layout <- function(boundingPoly = NULL, textSegmentType = NULL, textSegment = NULL, 
    pageNumber = NULL) {
    structure(list(boundingPoly = boundingPoly, textSegmentType = textSegmentType, 
        textSegment = textSegment, pageNumber = pageNumber), class = "gar_Layout")
}

#' DeployModelRequest Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Request message for AutoMl.DeployModel.
#' 
#' @param imageObjectDetectionModelDeploymentMetadata Model deployment metadata specific to Image Object Detection
#' @param imageClassificationModelDeploymentMetadata Model deployment metadata specific to Image Classification
#' 
#' @return DeployModelRequest object
#' 
#' @family DeployModelRequest functions
#' @export
DeployModelRequest <- function(imageObjectDetectionModelDeploymentMetadata = NULL, 
    imageClassificationModelDeploymentMetadata = NULL) {
    structure(list(imageObjectDetectionModelDeploymentMetadata = imageObjectDetectionModelDeploymentMetadata, 
        imageClassificationModelDeploymentMetadata = imageClassificationModelDeploymentMetadata), 
        class = "gar_DeployModelRequest")
}

#' WaitOperationRequest Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' The request message for Operations.WaitOperation.
#' 
#' @param timeout The maximum duration to wait before timing out
#' 
#' @return WaitOperationRequest object
#' 
#' @family WaitOperationRequest functions
#' @export
WaitOperationRequest <- function(timeout = NULL) {
    structure(list(timeout = timeout), class = "gar_WaitOperationRequest")
}

#' ImportDataRequest Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Request message for AutoMl.ImportData.
#' 
#' @param inputConfig Required
#' 
#' @return ImportDataRequest object
#' 
#' @family ImportDataRequest functions
#' @export
ImportDataRequest <- function(inputConfig = NULL) {
    structure(list(inputConfig = inputConfig), class = "gar_ImportDataRequest")
}

#' ExportDataOutputInfo Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Further describes this export data's output.SupplementsOutputConfig.
#' 
#' @param gcsOutputDirectory The full path of the Google Cloud Storage directory created, into which
#' 
#' @return ExportDataOutputInfo object
#' 
#' @family ExportDataOutputInfo functions
#' @export
ExportDataOutputInfo <- function(gcsOutputDirectory = NULL) {
    structure(list(gcsOutputDirectory = gcsOutputDirectory), class = "gar_ExportDataOutputInfo")
}

#' ImageClassificationDatasetMetadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Dataset metadata that is specific to image classification.
#' 
#' @param classificationType Required
#' 
#' @return ImageClassificationDatasetMetadata object
#' 
#' @family ImageClassificationDatasetMetadata functions
#' @export
ImageClassificationDatasetMetadata <- function(classificationType = NULL) {
    structure(list(classificationType = classificationType), class = "gar_ImageClassificationDatasetMetadata")
}

#' Expr Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Represents an expression text. Example:    title: 'User account presence'    description: 'Determines whether the request has a user account'    expression: 'size(request.user) > 0'
#' 
#' @param description An optional description of the expression
#' @param expression Textual representation of an expression in
#' @param title An optional title for the expression, i
#' @param location An optional string indicating the location of the expression for error
#' 
#' @return Expr object
#' 
#' @family Expr functions
#' @export
Expr <- function(description = NULL, expression = NULL, title = NULL, location = NULL) {
    structure(list(description = description, expression = expression, title = title, 
        location = location), class = "gar_Expr")
}

#' TextSnippet Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' A representation of a text snippet.
#' 
#' @param content Required
#' @param contentUri Output only
#' @param mimeType Optional
#' 
#' @return TextSnippet object
#' 
#' @family TextSnippet functions
#' @export
TextSnippet <- function(content = NULL, contentUri = NULL, mimeType = NULL) {
    structure(list(content = content, contentUri = contentUri, mimeType = mimeType), 
        class = "gar_TextSnippet")
}

#' TextExtractionEvaluationMetrics Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Model evaluation metrics for text extraction problems.
#' 
#' @param confidenceMetricsEntries Output only
#' @param auPrc Output only
#' 
#' @return TextExtractionEvaluationMetrics object
#' 
#' @family TextExtractionEvaluationMetrics functions
#' @export
TextExtractionEvaluationMetrics <- function(confidenceMetricsEntries = NULL, auPrc = NULL) {
    structure(list(confidenceMetricsEntries = confidenceMetricsEntries, auPrc = auPrc), 
        class = "gar_TextExtractionEvaluationMetrics")
}

#' UndeployModelRequest Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Request message for AutoMl.UndeployModel.
#' 
#' 
#' 
#' @return UndeployModelRequest object
#' 
#' @family UndeployModelRequest functions
#' @export
UndeployModelRequest <- function() {
    list()
}

#' TestIamPermissionsResponse Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Response message for `TestIamPermissions` method.
#' 
#' @param permissions A subset of `TestPermissionsRequest
#' 
#' @return TestIamPermissionsResponse object
#' 
#' @family TestIamPermissionsResponse functions
#' @export
TestIamPermissionsResponse <- function(permissions = NULL) {
    structure(list(permissions = permissions), class = "gar_TestIamPermissionsResponse")
}

#' PredictRequest Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Request message for PredictionService.Predict.
#' 
#' @param PredictRequest.params The \link{PredictRequest.params} object or list of objects
#' @param params Additional domain-specific parameters, any string must be up to 25000
#' @param payload Required
#' 
#' @return PredictRequest object
#' 
#' @family PredictRequest functions
#' @export
PredictRequest <- function(PredictRequest.params = NULL, params = NULL, payload = NULL) {
    structure(list(PredictRequest.params = PredictRequest.params, params = params, 
        payload = payload), class = "gar_PredictRequest")
}

#' PredictRequest.params Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Additional domain-specific parameters, any string must be up to 25000characters long.<h4>AutoML Vision Classification</h4>`score_threshold`: (float) A value from 0.0 to 1.0. When the model  makes predictions for an image, it will only produce results that have  at least this confidence score. The default is 0.5.<h4>AutoML Vision Object Detection</h4>`score_threshold`: (float) When Model detects objects on the image,  it will only produce bounding boxes which have at least this  confidence score. Value in 0 to 1 range, default is 0.5.`max_bounding_box_count`: (int64) The maximum number of bounding  boxes returned. The default is 100. The  number of returned bounding boxes might be limited by the server.<h4>AutoML Tables</h4>`feature_importance`: (boolean) Whetherfeature_importance  is populated in the returned list of  TablesAnnotation  objects. The default is false.
#' 
#' 
#' 
#' @return PredictRequest.params object
#' 
#' @family PredictRequest functions
#' @export
PredictRequest.params <- function() {
    list()
}

#' BatchPredictInputConfig Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Input configuration for BatchPredict Action.The format of input depends on the ML problem of the model used forprediction. As input source thegcs_sourceis expected, unless specified otherwise.The formats are represented in EBNF with commas being literal and withnon-terminal symbols defined near the end of this comment. The formatsare:<h4>AutoML Vision</h4><div class='ds-selector-tabs'><section><h5>Classification</h5>One or more CSV files where each line is a single column:    GCS_FILE_PATHThe Google Cloud Storage location of an image of up to30MB in size. Supported extensions: .JPEG, .GIF, .PNG.This path is treated as the ID in the batch predict output.Sample rows:    gs://folder/image1.jpeg    gs://folder/image2.gif    gs://folder/image3.png</section><section><h5>Object Detection</h5>One or more CSV files where each line is a single column:    GCS_FILE_PATHThe Google Cloud Storage location of an image of up to30MB in size. Supported extensions: .JPEG, .GIF, .PNG.This path is treated as the ID in the batch predict output.Sample rows:    gs://folder/image1.jpeg    gs://folder/image2.gif    gs://folder/image3.png  </section></div><h4>AutoML Video Intelligence</h4><div class='ds-selector-tabs'><section><h5>Classification</h5>One or more CSV files where each line is a single column:    GCS_FILE_PATH,TIME_SEGMENT_START,TIME_SEGMENT_END`GCS_FILE_PATH` is the Google Cloud Storage location of video up to 50GB insize and up to 3h in duration duration.Supported extensions: .MOV, .MPEG4, .MP4, .AVI.`TIME_SEGMENT_START` and `TIME_SEGMENT_END` must be within thelength of the video, and the end time must be after the start time.Sample rows:    gs://folder/video1.mp4,10,40    gs://folder/video1.mp4,20,60    gs://folder/vid2.mov,0,inf</section><section><h5>Object Tracking</h5>One or more CSV files where each line is a single column:    GCS_FILE_PATH,TIME_SEGMENT_START,TIME_SEGMENT_END`GCS_FILE_PATH` is the Google Cloud Storage location of video up to 50GB insize and up to 3h in duration duration.Supported extensions: .MOV, .MPEG4, .MP4, .AVI.`TIME_SEGMENT_START` and `TIME_SEGMENT_END` must be within thelength of the video, and the end time must be after the start time.Sample rows:    gs://folder/video1.mp4,10,40    gs://folder/video1.mp4,20,60    gs://folder/vid2.mov,0,inf  </section></div><h4>AutoML Natural Language</h4><div class='ds-selector-tabs'><section><h5>Classification</h5>One or more CSV files where each line is a single column:    GCS_FILE_PATH | TEXT_SNIPPET`GCS_FILE_PATH` is the Google Cloud Storage location of a text file.Supported file extensions: .TXT, .PDF`TEXT_SNIPPET` is inline text.Text files can be no larger than 10MB in size.Inline text content must be 60,000 characters or less.Sample rows:    gs://folder/text1.txt    'Some text content to predict'    gs://folder/text2.pdf</section><section><h5>Sentiment Analysis</h5>One or more CSV files where each line is a single column:    GCS_FILE_PATH | TEXT_SNIPPET`GCS_FILE_PATH` is the Google Cloud Storage location of a text file.Supported file extensions: .TXT, .PDF`TEXT_SNIPPET` is inline text.Text files can be no larger than 128kB in size.Inline text content must be 60,000 characters or less.Sample rows:    gs://folder/text1.txt    'Some text content to predict'    gs://folder/text2.pdf</section><section><h5>Entity Extraction</h5>One or more JSONL (JSON Lines) files that either provide inline text ordocuments. You can only use one format, either inline text or documents,for a single call to [AutoMl.BatchPredict].Each JSONL file contains a per line a proto thatwraps a temporary user-assigned TextSnippet ID (string up to 2000characters long) called 'id', a TextSnippet proto (inJSON representation) and zero or more TextFeature protos. Any giventext snippet content must have 30,000 characters or less, and alsobe UTF-8 NFC encoded (ASCII already is). The IDs provided should beunique.Each document JSONL file contains, per line, a proto that wraps aDocument proto with `input_config` set. Only PDF documents arecurrently supported, and each PDF document cannot exceed 2MB in size.Each JSONL file must not exceed 100MB in size, and no more than 20JSONL files may be passed.Sample inline JSONL file (Shown with artificial linebreaks. Actual line breaks are denoted by '\n'.):    {       'id': 'my_first_id',       'text_snippet': { 'content': 'dog car cat'},       'text_features': [         {           'text_segment': {'start_offset': 4, 'end_offset': 6},           'structural_type': PARAGRAPH,           'bounding_poly': {             'normalized_vertices': [               {'x': 0.1, 'y': 0.1},               {'x': 0.1, 'y': 0.3},               {'x': 0.3, 'y': 0.3},               {'x': 0.3, 'y': 0.1},             ]           },         }       ],     }\n     {       'id': '2',       'text_snippet': {         'content': 'Extended sample content',         'mime_type': 'text/plain'       }     }Sample document JSONL file (Shown with artificial linebreaks. Actual line breaks are denoted by '\n'.):     {       'document': {         'input_config': {           'gcs_source': { 'input_uris': [ 'gs://folder/document1.pdf' ]           }         }       }     }\n     {       'document': {         'input_config': {           'gcs_source': { 'input_uris': [ 'gs://folder/document2.pdf' ]           }         }       }     }  </section></div><h4>AutoML Tables</h4><div class='ui-datasection-main'><sectionclass='selected'>See [Preparing your trainingdata](https://cloud.google.com/automl-tables/docs/predict-batch) for moreinformation.You can use eithergcs_sourceorbigquery_source.**For gcs_source:**CSV file(s), each by itself 10GB or smaller and total size must be100GB or smaller, where first file must have a header containingcolumn names. If the first row of a subsequent file is the same asthe header, then it is also treated as a header. All other rowscontain values for the corresponding columns.The column names must contain the model'sinput_feature_column_specs'display_name-s(order doesn't matter). The columns corresponding to the model'sinput feature column specs must contain values compatible with thecolumn spec's data types. Prediction on all the rows, i.e. the CSVlines, will be attempted.Sample rows from a CSV file:<pre>'First Name','Last Name','Dob','Addresses''John','Doe','1968-01-22','[{'status':'current','address':'123_First_Avenue','city':'Seattle','state':'WA','zip':'11111','numberOfYears':'1'},{'status':'previous','address':'456_Main_Street','city':'Portland','state':'OR','zip':'22222','numberOfYears':'5'}]''Jane','Doe','1980-10-16','[{'status':'current','address':'789_Any_Avenue','city':'Albany','state':'NY','zip':'33333','numberOfYears':'2'},{'status':'previous','address':'321_Main_Street','city':'Hoboken','state':'NJ','zip':'44444','numberOfYears':'3'}]}</pre>**For bigquery_source:**The URI of a BigQuery table. The user data size of the BigQuerytable must be 100GB or smaller.The column names must contain the model'sinput_feature_column_specs'display_name-s(order doesn't matter). The columns corresponding to the model'sinput feature column specs must contain values compatible with thecolumn spec's data types. Prediction on all the rows of the tablewill be attempted.  </section></div>**Input field definitions:**`GCS_FILE_PATH`: The path to a file on Google Cloud Storage. For example,  'gs://folder/video.avi'.`TIME_SEGMENT_START`: (`TIME_OFFSET`)  Expresses a beginning, inclusive, of a time segment  within an example that has a time dimension  (e.g. video).`TIME_SEGMENT_END`: (`TIME_OFFSET`)  Expresses an end, exclusive, of a time segment within  n example that has a time dimension (e.g. video).`TIME_OFFSET`: A number of seconds as measured from the start of an  example (e.g. video). Fractions are allowed, up to a  microsecond precision. 'inf' is allowed, and it means the end  of the example. **Errors:** If any of the provided CSV files can't be parsed or if more than certain percent of CSV rows cannot be processed then the operation fails and prediction does not happen. Regardless of overall success or failure the per-row failures, up to a certain count cap, will be listed in Operation.metadata.partial_failures.
#' 
#' @param gcsSource Required
#' 
#' @return BatchPredictInputConfig object
#' 
#' @family BatchPredictInputConfig functions
#' @export
BatchPredictInputConfig <- function(gcsSource = NULL) {
    structure(list(gcsSource = gcsSource), class = "gar_BatchPredictInputConfig")
}

#' ListModelEvaluationsResponse Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Response message for AutoMl.ListModelEvaluations.
#' 
#' @param modelEvaluation List of model evaluations in the requested page
#' @param nextPageToken A token to retrieve next page of results
#' 
#' @return ListModelEvaluationsResponse object
#' 
#' @family ListModelEvaluationsResponse functions
#' @export
ListModelEvaluationsResponse <- function(modelEvaluation = NULL, nextPageToken = NULL) {
    structure(list(modelEvaluation = modelEvaluation, nextPageToken = nextPageToken), 
        class = "gar_ListModelEvaluationsResponse")
}

#' BatchPredictOperationMetadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Details of BatchPredict operation.
#' 
#' @param inputConfig Output only
#' @param outputInfo Output only
#' 
#' @return BatchPredictOperationMetadata object
#' 
#' @family BatchPredictOperationMetadata functions
#' @export
BatchPredictOperationMetadata <- function(inputConfig = NULL, outputInfo = NULL) {
    structure(list(inputConfig = inputConfig, outputInfo = outputInfo), class = "gar_BatchPredictOperationMetadata")
}

#' DocumentInputConfig Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Input configuration of a Document.
#' 
#' @param gcsSource The Google Cloud Storage location of the document file
#' 
#' @return DocumentInputConfig object
#' 
#' @family DocumentInputConfig functions
#' @export
DocumentInputConfig <- function(gcsSource = NULL) {
    structure(list(gcsSource = gcsSource), class = "gar_DocumentInputConfig")
}

#' AnnotationPayload Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Contains annotation information that is relevant to AutoML.
#' 
#' @param annotationSpecId Output only 
#' @param classification Annotation details for content or image classification
#' @param textExtraction Annotation details for text extraction
#' @param imageObjectDetection Annotation details for image object detection
#' @param displayName Output only
#' @param translation Annotation details for translation
#' @param textSentiment Annotation details for text sentiment
#' 
#' @return AnnotationPayload object
#' 
#' @family AnnotationPayload functions
#' @export
AnnotationPayload <- function(annotationSpecId = NULL, classification = NULL, textExtraction = NULL, 
    imageObjectDetection = NULL, displayName = NULL, translation = NULL, textSentiment = NULL) {
    structure(list(annotationSpecId = annotationSpecId, classification = classification, 
        textExtraction = textExtraction, imageObjectDetection = imageObjectDetection, 
        displayName = displayName, translation = translation, textSentiment = textSentiment), 
        class = "gar_AnnotationPayload")
}

#' Dataset Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' A workspace for solving a single, particular machine learning (ML) problem.A workspace contains examples that may be annotated.
#' 
#' @param Dataset.labels The \link{Dataset.labels} object or list of objects
#' @param translationDatasetMetadata Metadata for a dataset used for translation
#' @param exampleCount Output only
#' @param textClassificationDatasetMetadata Metadata for a dataset used for text classification
#' @param displayName Required
#' @param createTime Output only
#' @param labels Optional
#' @param textExtractionDatasetMetadata Metadata for a dataset used for text extraction
#' @param description User-provided description of the dataset
#' @param imageClassificationDatasetMetadata Metadata for a dataset used for image classification
#' @param etag Used to perform consistent read-modify-write updates
#' @param textSentimentDatasetMetadata Metadata for a dataset used for text sentiment
#' @param name Output only
#' @param imageObjectDetectionDatasetMetadata Metadata for a dataset used for image object detection
#' 
#' @return Dataset object
#' 
#' @family Dataset functions
#' @export
Dataset <- function(Dataset.labels = NULL, translationDatasetMetadata = NULL, exampleCount = NULL, 
    textClassificationDatasetMetadata = NULL, displayName = NULL, createTime = NULL, 
    labels = NULL, textExtractionDatasetMetadata = NULL, description = NULL, imageClassificationDatasetMetadata = NULL, 
    etag = NULL, textSentimentDatasetMetadata = NULL, name = NULL, imageObjectDetectionDatasetMetadata = NULL) {
    structure(list(Dataset.labels = Dataset.labels, translationDatasetMetadata = translationDatasetMetadata, 
        exampleCount = exampleCount, textClassificationDatasetMetadata = textClassificationDatasetMetadata, 
        displayName = displayName, createTime = createTime, labels = labels, textExtractionDatasetMetadata = textExtractionDatasetMetadata, 
        description = description, imageClassificationDatasetMetadata = imageClassificationDatasetMetadata, 
        etag = etag, textSentimentDatasetMetadata = textSentimentDatasetMetadata, 
        name = name, imageObjectDetectionDatasetMetadata = imageObjectDetectionDatasetMetadata), 
        class = "gar_Dataset")
}

#' Dataset.labels Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Optional. The labels with user-defined metadata to organize your dataset.Label keys and values can be no longer than 64 characters(Unicode codepoints), can only contain lowercase letters, numericcharacters, underscores and dashes. International characters are allowed.Label values are optional. Label keys must start with a letter.See https://goo.gl/xmQnxf for more information on and examples of labels.
#' 
#' 
#' 
#' @return Dataset.labels object
#' 
#' @family Dataset functions
#' @export
Dataset.labels <- function() {
    list()
}

#' TextSentimentEvaluationMetrics Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Model evaluation metrics for text sentiment problems.
#' 
#' @param quadraticKappa Output only
#' @param meanAbsoluteError Output only
#' @param recall Output only
#' @param linearKappa Output only
#' @param f1Score Output only
#' @param precision Output only
#' @param meanSquaredError Output only
#' @param confusionMatrix Output only
#' 
#' @return TextSentimentEvaluationMetrics object
#' 
#' @family TextSentimentEvaluationMetrics functions
#' @export
TextSentimentEvaluationMetrics <- function(quadraticKappa = NULL, meanAbsoluteError = NULL, 
    recall = NULL, linearKappa = NULL, f1Score = NULL, precision = NULL, meanSquaredError = NULL, 
    confusionMatrix = NULL) {
    structure(list(quadraticKappa = quadraticKappa, meanAbsoluteError = meanAbsoluteError, 
        recall = recall, linearKappa = linearKappa, f1Score = f1Score, precision = precision, 
        meanSquaredError = meanSquaredError, confusionMatrix = confusionMatrix), 
        class = "gar_TextSentimentEvaluationMetrics")
}

#' TranslationEvaluationMetrics Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Evaluation metrics for the dataset.
#' 
#' @param bleuScore Output only
#' @param baseBleuScore Output only
#' 
#' @return TranslationEvaluationMetrics object
#' 
#' @family TranslationEvaluationMetrics functions
#' @export
TranslationEvaluationMetrics <- function(bleuScore = NULL, baseBleuScore = NULL) {
    structure(list(bleuScore = bleuScore, baseBleuScore = baseBleuScore), class = "gar_TranslationEvaluationMetrics")
}

#' ClassificationEvaluationMetricsConfidenceMetricsEntry Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Metrics for a single confidence threshold.
#' 
#' @param recallAt1 Output only
#' @param f1ScoreAt1 Output only
#' @param precision Output only
#' @param truePositiveCount Output only
#' @param precisionAt1 Output only
#' @param confidenceThreshold Output only
#' @param f1Score Output only
#' @param trueNegativeCount Output only
#' @param positionThreshold Output only
#' @param falseNegativeCount Output only
#' @param falsePositiveCount Output only
#' @param falsePositiveRate Output only
#' @param recall Output only
#' @param falsePositiveRateAt1 Output only
#' 
#' @return ClassificationEvaluationMetricsConfidenceMetricsEntry object
#' 
#' @family ClassificationEvaluationMetricsConfidenceMetricsEntry functions
#' @export
ClassificationEvaluationMetricsConfidenceMetricsEntry <- function(recallAt1 = NULL, 
    f1ScoreAt1 = NULL, precision = NULL, truePositiveCount = NULL, precisionAt1 = NULL, 
    confidenceThreshold = NULL, f1Score = NULL, trueNegativeCount = NULL, positionThreshold = NULL, 
    falseNegativeCount = NULL, falsePositiveCount = NULL, falsePositiveRate = NULL, 
    recall = NULL, falsePositiveRateAt1 = NULL) {
    structure(list(recallAt1 = recallAt1, f1ScoreAt1 = f1ScoreAt1, precision = precision, 
        truePositiveCount = truePositiveCount, precisionAt1 = precisionAt1, confidenceThreshold = confidenceThreshold, 
        f1Score = f1Score, trueNegativeCount = trueNegativeCount, positionThreshold = positionThreshold, 
        falseNegativeCount = falseNegativeCount, falsePositiveCount = falsePositiveCount, 
        falsePositiveRate = falsePositiveRate, recall = recall, falsePositiveRateAt1 = falsePositiveRateAt1), 
        class = "gar_ClassificationEvaluationMetricsConfidenceMetricsEntry")
}

#' ListModelsResponse Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Response message for AutoMl.ListModels.
#' 
#' @param model List of models in the requested page
#' @param nextPageToken A token to retrieve next page of results
#' 
#' @return ListModelsResponse object
#' 
#' @family ListModelsResponse functions
#' @export
ListModelsResponse <- function(model = NULL, nextPageToken = NULL) {
    structure(list(model = model, nextPageToken = nextPageToken), class = "gar_ListModelsResponse")
}

#' DeleteOperationMetadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Details of operations that perform deletes of any entities.
#' 
#' 
#' 
#' @return DeleteOperationMetadata object
#' 
#' @family DeleteOperationMetadata functions
#' @export
DeleteOperationMetadata <- function() {
    list()
}

#' ExportDataOperationMetadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Details of ExportData operation.
#' 
#' @param outputInfo Output only
#' 
#' @return ExportDataOperationMetadata object
#' 
#' @family ExportDataOperationMetadata functions
#' @export
ExportDataOperationMetadata <- function(outputInfo = NULL) {
    structure(list(outputInfo = outputInfo), class = "gar_ExportDataOperationMetadata")
}

#' OutputConfig Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' *  For Translation:        CSV file `translation.csv`, with each line in format:        ML_USE,GCS_FILE_PATH        GCS_FILE_PATH leads to a .TSV file which describes examples that have        given ML_USE, using the following row format per line:        TEXT_SNIPPET (in source language) \t TEXT_SNIPPET (in target        language)  *  For Tables:        Output depends on whether the dataset was imported from Google Cloud        Storage or BigQuery.        Google Cloud Storage case:gcs_destination          must be set. Exported are CSV file(s) `tables_1.csv`,          `tables_2.csv`,...,`tables_N.csv` with each having as header line          the table's column names, and all other lines contain values for          the header columns.        BigQuery case:bigquery_destination          pointing to a BigQuery project must be set. In the given project a          new dataset will be created with name`export_data_<automl-dataset-display-name>_<timestamp-of-export-call>`          where <automl-dataset-display-name> will be made          BigQuery-dataset-name compatible (e.g. most special characters will          become underscores), and timestamp will be in          YYYY_MM_DDThh_mm_ss_sssZ 'based on ISO-8601' format. In that          dataset a new table called `primary_table` will be created, and          filled with precisely the same data as this obtained on import.
#' 
#' @param gcsDestination Required
#' 
#' @return OutputConfig object
#' 
#' @family OutputConfig functions
#' @export
OutputConfig <- function(gcsDestination = NULL) {
    structure(list(gcsDestination = gcsDestination), class = "gar_OutputConfig")
}

#' TextExtractionEvaluationMetricsConfidenceMetricsEntry Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Metrics for a single confidence threshold.
#' 
#' @param recall Output only
#' @param f1Score Output only
#' @param precision Output only
#' @param confidenceThreshold Output only
#' 
#' @return TextExtractionEvaluationMetricsConfidenceMetricsEntry object
#' 
#' @family TextExtractionEvaluationMetricsConfidenceMetricsEntry functions
#' @export
TextExtractionEvaluationMetricsConfidenceMetricsEntry <- function(recall = NULL, 
    f1Score = NULL, precision = NULL, confidenceThreshold = NULL) {
    structure(list(recall = recall, f1Score = f1Score, precision = precision, confidenceThreshold = confidenceThreshold), 
        class = "gar_TextExtractionEvaluationMetricsConfidenceMetricsEntry")
}

#' TextClassificationModelMetadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Model metadata that is specific to text classification.
#' 
#' @param classificationType Output only
#' 
#' @return TextClassificationModelMetadata object
#' 
#' @family TextClassificationModelMetadata functions
#' @export
TextClassificationModelMetadata <- function(classificationType = NULL) {
    structure(list(classificationType = classificationType), class = "gar_TextClassificationModelMetadata")
}

#' NormalizedVertex Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' A vertex represents a 2D point in the image.The normalized vertex coordinates are between 0 to 1 fractions relative tothe original plane (image, video). E.g. if the plane (e.g. whole image) wouldhave size 10 x 20 then a point with normalized coordinates (0.1, 0.3) wouldbe at the position (1, 6) on that plane.
#' 
#' @param y Required
#' @param x Required
#' 
#' @return NormalizedVertex object
#' 
#' @family NormalizedVertex functions
#' @export
NormalizedVertex <- function(y = NULL, x = NULL) {
    structure(list(y = y, x = x), class = "gar_NormalizedVertex")
}

#' Empty Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' A generic empty message that you can re-use to avoid defining duplicatedempty messages in your APIs. A typical example is to use it as the requestor the response type of an API method. For instance:    service Foo {      rpc Bar(google.protobuf.Empty) returns (google.protobuf.Empty);    }The JSON representation for `Empty` is empty JSON object `{}`.
#' 
#' 
#' 
#' @return Empty object
#' 
#' @family Empty functions
#' @export
Empty <- function() {
    list()
}

#' TextExtractionAnnotation Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Annotation for identifying spans of text.
#' 
#' @param score Output only
#' @param textSegment An entity annotation will set this, which is the part of the original
#' 
#' @return TextExtractionAnnotation object
#' 
#' @family TextExtractionAnnotation functions
#' @export
TextExtractionAnnotation <- function(score = NULL, textSegment = NULL) {
    structure(list(score = score, textSegment = textSegment), class = "gar_TextExtractionAnnotation")
}

#' ImageObjectDetectionModelDeploymentMetadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Model deployment metadata specific to Image Object Detection.
#' 
#' @param nodeCount Input only
#' 
#' @return ImageObjectDetectionModelDeploymentMetadata object
#' 
#' @family ImageObjectDetectionModelDeploymentMetadata functions
#' @export
ImageObjectDetectionModelDeploymentMetadata <- function(nodeCount = NULL) {
    structure(list(nodeCount = nodeCount), class = "gar_ImageObjectDetectionModelDeploymentMetadata")
}

#' TextSegment Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' A contiguous part of a text (string), assuming it has an UTF-8 NFC encoding.
#' 
#' @param endOffset Required
#' @param startOffset Required
#' @param content Output only
#' 
#' @return TextSegment object
#' 
#' @family TextSegment functions
#' @export
TextSegment <- function(endOffset = NULL, startOffset = NULL, content = NULL) {
    structure(list(endOffset = endOffset, startOffset = startOffset, content = content), 
        class = "gar_TextSegment")
}

#' ConfusionMatrix Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Confusion matrix of the model running the classification.
#' 
#' @param displayName Output only
#' @param annotationSpecId Output only
#' @param row Output only
#' 
#' @return ConfusionMatrix object
#' 
#' @family ConfusionMatrix functions
#' @export
ConfusionMatrix <- function(displayName = NULL, annotationSpecId = NULL, row = NULL) {
    structure(list(displayName = displayName, annotationSpecId = annotationSpecId, 
        row = row), class = "gar_ConfusionMatrix")
}

#' GcsDestination Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' The Google Cloud Storage location where the output is to be written to.
#' 
#' @param outputUriPrefix Required
#' 
#' @return GcsDestination object
#' 
#' @family GcsDestination functions
#' @export
GcsDestination <- function(outputUriPrefix = NULL) {
    structure(list(outputUriPrefix = outputUriPrefix), class = "gar_GcsDestination")
}

#' TextSentimentAnnotation Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Contains annotation details specific to text sentiment.
#' 
#' @param sentiment Output only
#' 
#' @return TextSentimentAnnotation object
#' 
#' @family TextSentimentAnnotation functions
#' @export
TextSentimentAnnotation <- function(sentiment = NULL) {
    structure(list(sentiment = sentiment), class = "gar_TextSentimentAnnotation")
}

#' AnnotationSpec Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' A definition of an annotation spec.
#' 
#' @param name Output only
#' @param displayName Required
#' 
#' @return AnnotationSpec object
#' 
#' @family AnnotationSpec functions
#' @export
AnnotationSpec <- function(name = NULL, displayName = NULL) {
    structure(list(name = name, displayName = displayName), class = "gar_AnnotationSpec")
}

#' Policy Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' An Identity and Access Management (IAM) policy, which specifies accesscontrols for Google Cloud resources.A `Policy` is a collection of `bindings`. A `binding` binds one or more`members` to a single `role`. Members can be user accounts, service accounts,Google groups, and domains (such as G Suite). A `role` is a named list ofpermissions; each `role` can be an IAM predefined role or a user-createdcustom role.Optionally, a `binding` can specify a `condition`, which is a logicalexpression that allows access to a resource only if the expression evaluatesto `true`. A condition can add constraints based on attributes of therequest, the resource, or both.**JSON example:**    {      'bindings': [        {          'role': 'roles/resourcemanager.organizationAdmin',          'members': [            'user:mike@example.com',            'group:admins@example.com',            'domain:google.com',            'serviceAccount:my-project-id@appspot.gserviceaccount.com'          ]        },        {          'role': 'roles/resourcemanager.organizationViewer',          'members': ['user:eve@example.com'],          'condition': {            'title': 'expirable access',            'description': 'Does not grant access after Sep 2020',            'expression': 'request.time < timestamp('2020-10-01T00:00:00.000Z')',          }        }      ],      'etag': 'BwWWja0YfJA=',      'version': 3    }**YAML example:**    bindings:    - members:      - user:mike@example.com      - group:admins@example.com      - domain:google.com      - serviceAccount:my-project-id@appspot.gserviceaccount.com      role: roles/resourcemanager.organizationAdmin    - members:      - user:eve@example.com      role: roles/resourcemanager.organizationViewer      condition:        title: expirable access        description: Does not grant access after Sep 2020        expression: request.time < timestamp('2020-10-01T00:00:00.000Z')    - etag: BwWWja0YfJA=    - version: 3For a description of IAM and its features, see the[IAM documentation](https://cloud.google.com/iam/docs/).
#' 
#' @param bindings Associates a list of `members` to a `role`
#' @param etag `etag` is used for optimistic concurrency control as a way to help
#' @param version Specifies the format of the policy
#' 
#' @return Policy object
#' 
#' @family Policy functions
#' @export
Policy <- function(bindings = NULL, etag = NULL, version = NULL) {
    structure(list(bindings = bindings, etag = etag, version = version), class = "gar_Policy")
}

#' ListLocationsResponse Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' The response message for Locations.ListLocations.
#' 
#' @param locations A list of locations that matches the specified filter in the request
#' @param nextPageToken The standard List next-page token
#' 
#' @return ListLocationsResponse object
#' 
#' @family ListLocationsResponse functions
#' @export
ListLocationsResponse <- function(locations = NULL, nextPageToken = NULL) {
    structure(list(locations = locations, nextPageToken = nextPageToken), class = "gar_ListLocationsResponse")
}

#' Model Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' API proto representing a trained machine learning model.
#' 
#' @param Model.labels The \link{Model.labels} object or list of objects
#' @param name Output only
#' @param translationModelMetadata Metadata for translation models
#' @param displayName Required
#' @param deploymentState Output only
#' @param createTime Output only
#' @param imageClassificationModelMetadata Metadata for image classification models
#' @param labels Optional
#' @param textExtractionModelMetadata Metadata for text extraction models
#' @param updateTime Output only
#' @param textClassificationModelMetadata Metadata for text classification models
#' @param etag Used to perform a consistent read-modify-write updates
#' @param imageObjectDetectionModelMetadata Metadata for image object detection models
#' @param textSentimentModelMetadata Metadata for text sentiment models
#' @param datasetId Required
#' 
#' @return Model object
#' 
#' @family Model functions
#' @export
Model <- function(Model.labels = NULL, name = NULL, translationModelMetadata = NULL, 
    displayName = NULL, deploymentState = NULL, createTime = NULL, imageClassificationModelMetadata = NULL, 
    labels = NULL, textExtractionModelMetadata = NULL, updateTime = NULL, textClassificationModelMetadata = NULL, 
    etag = NULL, imageObjectDetectionModelMetadata = NULL, textSentimentModelMetadata = NULL, 
    datasetId = NULL) {
    structure(list(Model.labels = Model.labels, name = name, translationModelMetadata = translationModelMetadata, 
        displayName = displayName, deploymentState = deploymentState, createTime = createTime, 
        imageClassificationModelMetadata = imageClassificationModelMetadata, labels = labels, 
        textExtractionModelMetadata = textExtractionModelMetadata, updateTime = updateTime, 
        textClassificationModelMetadata = textClassificationModelMetadata, etag = etag, 
        imageObjectDetectionModelMetadata = imageObjectDetectionModelMetadata, textSentimentModelMetadata = textSentimentModelMetadata, 
        datasetId = datasetId), class = "gar_Model")
}

#' Model.labels Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Optional. The labels with user-defined metadata to organize your model.Label keys and values can be no longer than 64 characters(Unicode codepoints), can only contain lowercase letters, numericcharacters, underscores and dashes. International characters are allowed.Label values are optional. Label keys must start with a letter.See https://goo.gl/xmQnxf for more information on and examples of labels.
#' 
#' 
#' 
#' @return Model.labels object
#' 
#' @family Model functions
#' @export
Model.labels <- function() {
    list()
}

#' TranslationAnnotation Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Annotation details specific to translation.
#' 
#' @param translatedContent Output only 
#' 
#' @return TranslationAnnotation object
#' 
#' @family TranslationAnnotation functions
#' @export
TranslationAnnotation <- function(translatedContent = NULL) {
    structure(list(translatedContent = translatedContent), class = "gar_TranslationAnnotation")
}

#' CancelOperationRequest Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' The request message for Operations.CancelOperation.
#' 
#' 
#' 
#' @return CancelOperationRequest object
#' 
#' @family CancelOperationRequest functions
#' @export
CancelOperationRequest <- function() {
    list()
}

#' ExportModelRequest Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Request message for AutoMl.ExportModel.Models need to be enabled for exporting, otherwise an error code will bereturned.
#' 
#' @param outputConfig Required
#' 
#' @return ExportModelRequest object
#' 
#' @family ExportModelRequest functions
#' @export
ExportModelRequest <- function(outputConfig = NULL) {
    structure(list(outputConfig = outputConfig), class = "gar_ExportModelRequest")
}

#' DocumentDimensions Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Message that describes dimension of a document.
#' 
#' @param height Height value of the document, works together with the unit
#' @param width Width value of the document, works together with the unit
#' @param unit Unit of the dimension
#' 
#' @return DocumentDimensions object
#' 
#' @family DocumentDimensions functions
#' @export
DocumentDimensions <- function(height = NULL, width = NULL, unit = NULL) {
    structure(list(height = height, width = width, unit = unit), class = "gar_DocumentDimensions")
}

#' Operation Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' This resource represents a long-running operation that is the result of anetwork API call.
#' 
#' @param Operation.response The \link{Operation.response} object or list of objects
#' @param Operation.metadata The \link{Operation.metadata} object or list of objects
#' @param done If the value is `false`, it means the operation is still in progress
#' @param response The normal response of the operation in case of success
#' @param name The server-assigned name, which is only unique within the same service that
#' @param error The error result of the operation in case of failure or cancellation
#' @param metadata Service-specific metadata associated with the operation
#' 
#' @return Operation object
#' 
#' @family Operation functions
#' @export
Operation <- function(Operation.response = NULL, Operation.metadata = NULL, done = NULL, 
    response = NULL, name = NULL, error = NULL, metadata = NULL) {
    structure(list(Operation.response = Operation.response, Operation.metadata = Operation.metadata, 
        done = done, response = response, name = name, error = error, metadata = metadata), 
        class = "gar_Operation")
}

#' Operation.response Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' The normal response of the operation in case of success.  If the originalmethod returns no data on success, such as `Delete`, the response is`google.protobuf.Empty`.  If the original method is standard`Get`/`Create`/`Update`, the response should be the resource.  For othermethods, the response should have the type `XxxResponse`, where `Xxx`is the original method name.  For example, if the original method nameis `TakeSnapshot()`, the inferred response type is`TakeSnapshotResponse`.
#' 
#' 
#' 
#' @return Operation.response object
#' 
#' @family Operation functions
#' @export
Operation.response <- function() {
    list()
}

#' Operation.metadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Service-specific metadata associated with the operation.  It typicallycontains progress information and common metadata such as create time.Some services might not provide such metadata.  Any method that returns along-running operation should document the metadata type, if any.
#' 
#' 
#' 
#' @return Operation.metadata object
#' 
#' @family Operation functions
#' @export
Operation.metadata <- function() {
    list()
}

#' CreateDatasetOperationMetadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Details of CreateDataset operation.
#' 
#' 
#' 
#' @return CreateDatasetOperationMetadata object
#' 
#' @family CreateDatasetOperationMetadata functions
#' @export
CreateDatasetOperationMetadata <- function() {
    list()
}

#' TextSentimentModelMetadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Model metadata that is specific to text sentiment.
#' 
#' 
#' 
#' @return TextSentimentModelMetadata object
#' 
#' @family TextSentimentModelMetadata functions
#' @export
TextSentimentModelMetadata <- function() {
    list()
}

#' ModelExportOutputConfig Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Output configuration for ModelExport Action.
#' 
#' @param ModelExportOutputConfig.params The \link{ModelExportOutputConfig.params} object or list of objects
#' @param modelFormat The format in which the model must be exported
#' @param params Additional model-type and format specific parameters describing the
#' @param gcsDestination Required
#' 
#' @return ModelExportOutputConfig object
#' 
#' @family ModelExportOutputConfig functions
#' @export
ModelExportOutputConfig <- function(ModelExportOutputConfig.params = NULL, modelFormat = NULL, 
    params = NULL, gcsDestination = NULL) {
    structure(list(ModelExportOutputConfig.params = ModelExportOutputConfig.params, 
        modelFormat = modelFormat, params = params, gcsDestination = gcsDestination), 
        class = "gar_ModelExportOutputConfig")
}

#' ModelExportOutputConfig.params Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Additional model-type and format specific parameters describing therequirements for the to be exported model files, any string must be up to25000 characters long. * For `docker` format:    `cpu_architecture` - (string) 'x86_64' (default).    `gpu_architecture` - (string) 'none' (default), 'nvidia'.
#' 
#' 
#' 
#' @return ModelExportOutputConfig.params object
#' 
#' @family ModelExportOutputConfig functions
#' @export
ModelExportOutputConfig.params <- function() {
    list()
}

#' ExamplePayload Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Example data used for training or prediction.
#' 
#' @param textSnippet Example text
#' @param document Example document
#' @param image Example image
#' 
#' @return ExamplePayload object
#' 
#' @family ExamplePayload functions
#' @export
ExamplePayload <- function(textSnippet = NULL, document = NULL, image = NULL) {
    structure(list(textSnippet = textSnippet, document = document, image = image), 
        class = "gar_ExamplePayload")
}

#' ModelEvaluation Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Evaluation results of a model.
#' 
#' @param evaluatedExampleCount Output only
#' @param name Output only
#' @param textSentimentEvaluationMetrics Evaluation metrics for text sentiment models
#' @param imageObjectDetectionEvaluationMetrics Model evaluation metrics for image object detection
#' @param translationEvaluationMetrics Model evaluation metrics for translation
#' @param annotationSpecId Output only
#' @param textExtractionEvaluationMetrics Evaluation metrics for text extraction models
#' @param displayName Output only
#' @param classificationEvaluationMetrics Model evaluation metrics for image, text, video and tables
#' @param createTime Output only
#' 
#' @return ModelEvaluation object
#' 
#' @family ModelEvaluation functions
#' @export
ModelEvaluation <- function(evaluatedExampleCount = NULL, name = NULL, textSentimentEvaluationMetrics = NULL, 
    imageObjectDetectionEvaluationMetrics = NULL, translationEvaluationMetrics = NULL, 
    annotationSpecId = NULL, textExtractionEvaluationMetrics = NULL, displayName = NULL, 
    classificationEvaluationMetrics = NULL, createTime = NULL) {
    structure(list(evaluatedExampleCount = evaluatedExampleCount, name = name, textSentimentEvaluationMetrics = textSentimentEvaluationMetrics, 
        imageObjectDetectionEvaluationMetrics = imageObjectDetectionEvaluationMetrics, 
        translationEvaluationMetrics = translationEvaluationMetrics, annotationSpecId = annotationSpecId, 
        textExtractionEvaluationMetrics = textExtractionEvaluationMetrics, displayName = displayName, 
        classificationEvaluationMetrics = classificationEvaluationMetrics, createTime = createTime), 
        class = "gar_ModelEvaluation")
}

#' Status Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' The `Status` type defines a logical error model that is suitable fordifferent programming environments, including REST APIs and RPC APIs. It isused by [gRPC](https://github.com/grpc). Each `Status` message containsthree pieces of data: error code, error message, and error details.You can find out more about this error model and how to work with it in the[API Design Guide](https://cloud.google.com/apis/design/errors).
#' 
#' @param code The status code, which should be an enum value of google
#' @param message A developer-facing error message, which should be in English
#' @param details A list of messages that carry the error details
#' 
#' @return Status object
#' 
#' @family Status functions
#' @export
Status <- function(code = NULL, message = NULL, details = NULL) {
    structure(list(code = code, message = message, details = details), class = "gar_Status")
}

#' Binding Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Associates `members` with a `role`.
#' 
#' @param role Role that is assigned to `members`
#' @param condition The condition that is associated with this binding
#' @param members Specifies the identities requesting access for a Cloud Platform resource
#' 
#' @return Binding object
#' 
#' @family Binding functions
#' @export
Binding <- function(role = NULL, condition = NULL, members = NULL) {
    structure(list(role = role, condition = condition, members = members), class = "gar_Binding")
}

#' BatchPredictOutputConfig Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Output configuration for BatchPredict Action.As destination thegcs_destinationmust be set unless specified otherwise for a domain. If gcs_destination isset then in the given directory a new directory is created. Its namewill be'prediction-<model-display-name>-<timestamp-of-prediction-call>',where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format. The contentsof it depends on the ML problem the predictions are made for. *  For Image Classification:        In the created directory files `image_classification_1.jsonl`,        `image_classification_2.jsonl`,...,`image_classification_N.jsonl`        will be created, where N may be 1, and depends on the        total number of the successfully predicted images and annotations.        A single image will be listed only once with all its annotations,        and its annotations will never be split across files.        Each .JSONL file will contain, per line, a JSON representation of a        proto that wraps image's 'ID' : '<id_value>' followed by a list of        zero or more AnnotationPayload protos (called annotations), which        have classification detail populated.        If prediction for any image failed (partially or completely), then an        additional `errors_1.jsonl`, `errors_2.jsonl`,..., `errors_N.jsonl`        files will be created (N depends on total number of failed        predictions). These files will have a JSON representation of a proto        that wraps the same 'ID' : '<id_value>' but here followed by        exactly one[`google.rpc.Status`](https://github.com/googleapis/googleapis/blob/master/google/rpc/status.proto)        containing only `code` and `message`fields. *  For Image Object Detection:        In the created directory files `image_object_detection_1.jsonl`,        `image_object_detection_2.jsonl`,...,`image_object_detection_N.jsonl`        will be created, where N may be 1, and depends on the        total number of the successfully predicted images and annotations.        Each .JSONL file will contain, per line, a JSON representation of a        proto that wraps image's 'ID' : '<id_value>' followed by a list of        zero or more AnnotationPayload protos (called annotations), which        have image_object_detection detail populated. A single image will        be listed only once with all its annotations, and its annotations        will never be split across files.        If prediction for any image failed (partially or completely), then        additional `errors_1.jsonl`, `errors_2.jsonl`,..., `errors_N.jsonl`        files will be created (N depends on total number of failed        predictions). These files will have a JSON representation of a proto        that wraps the same 'ID' : '<id_value>' but here followed by        exactly one[`google.rpc.Status`](https://github.com/googleapis/googleapis/blob/master/google/rpc/status.proto)        containing only `code` and `message`fields. *  For Video Classification:        In the created directory a video_classification.csv file, and a .JSON        file per each video classification requested in the input (i.e. each        line in given CSV(s)), will be created.        The format of video_classification.csv is:GCS_FILE_PATH,TIME_SEGMENT_START,TIME_SEGMENT_END,JSON_FILE_NAME,STATUS        where:        GCS_FILE_PATH,TIME_SEGMENT_START,TIME_SEGMENT_END = matches 1 to 1            the prediction input lines (i.e. video_classification.csv has            precisely the same number of lines as the prediction input had.)        JSON_FILE_NAME = Name of .JSON file in the output directory, which            contains prediction responses for the video time segment.        STATUS = 'OK' if prediction completed successfully, or an error code            with message otherwise. If STATUS is not 'OK' then the .JSON file            for that line may not exist or be empty.        Each .JSON file, assuming STATUS is 'OK', will contain a list of        AnnotationPayload protos in JSON format, which are the predictions        for the video time segment the file is assigned to in the        video_classification.csv. All AnnotationPayload protos will have        video_classification field set, and will be sorted by        video_classification.type field (note that the returned types are        governed by `classifaction_types` parameter in        PredictService.BatchPredictRequest.params). *  For Video Object Tracking:        In the created directory a video_object_tracking.csv file will be        created, and multiple files video_object_trackinng_1.json,        video_object_trackinng_2.json,..., video_object_trackinng_N.json,        where N is the number of requests in the input (i.e. the number of        lines in given CSV(s)).        The format of video_object_tracking.csv is:GCS_FILE_PATH,TIME_SEGMENT_START,TIME_SEGMENT_END,JSON_FILE_NAME,STATUS        where:        GCS_FILE_PATH,TIME_SEGMENT_START,TIME_SEGMENT_END = matches 1 to 1            the prediction input lines (i.e. video_object_tracking.csv has            precisely the same number of lines as the prediction input had.)        JSON_FILE_NAME = Name of .JSON file in the output directory, which            contains prediction responses for the video time segment.        STATUS = 'OK' if prediction completed successfully, or an error            code with message otherwise. If STATUS is not 'OK' then the .JSON            file for that line may not exist or be empty.        Each .JSON file, assuming STATUS is 'OK', will contain a list of        AnnotationPayload protos in JSON format, which are the predictions        for each frame of the video time segment the file is assigned to in        video_object_tracking.csv. All AnnotationPayload protos will have        video_object_tracking field set. *  For Text Classification:        In the created directory files `text_classification_1.jsonl`,        `text_classification_2.jsonl`,...,`text_classification_N.jsonl`        will be created, where N may be 1, and depends on the        total number of inputs and annotations found.        Each .JSONL file will contain, per line, a JSON representation of a        proto that wraps input text (or pdf) file in        the text snippet (or document) proto and a list of        zero or more AnnotationPayload protos (called annotations), which        have classification detail populated. A single text (or pdf) file        will be listed only once with all its annotations, and its        annotations will never be split across files.        If prediction for any text (or pdf) file failed (partially or        completely), then additional `errors_1.jsonl`, `errors_2.jsonl`,...,        `errors_N.jsonl` files will be created (N depends on total number of        failed predictions). These files will have a JSON representation of a        proto that wraps input text (or pdf) file followed by exactly one[`google.rpc.Status`](https://github.com/googleapis/googleapis/blob/master/google/rpc/status.proto)        containing only `code` and `message`. *  For Text Sentiment:        In the created directory files `text_sentiment_1.jsonl`,        `text_sentiment_2.jsonl`,...,`text_sentiment_N.jsonl`        will be created, where N may be 1, and depends on the        total number of inputs and annotations found.        Each .JSONL file will contain, per line, a JSON representation of a        proto that wraps input text (or pdf) file in        the text snippet (or document) proto and a list of        zero or more AnnotationPayload protos (called annotations), which        have text_sentiment detail populated. A single text (or pdf) file        will be listed only once with all its annotations, and its        annotations will never be split across files.        If prediction for any text (or pdf) file failed (partially or        completely), then additional `errors_1.jsonl`, `errors_2.jsonl`,...,        `errors_N.jsonl` files will be created (N depends on total number of        failed predictions). These files will have a JSON representation of a        proto that wraps input text (or pdf) file followed by exactly one[`google.rpc.Status`](https://github.com/googleapis/googleapis/blob/master/google/rpc/status.proto)        containing only `code` and `message`.  *  For Text Extraction:        In the created directory files `text_extraction_1.jsonl`,        `text_extraction_2.jsonl`,...,`text_extraction_N.jsonl`        will be created, where N may be 1, and depends on the        total number of inputs and annotations found.        The contents of these .JSONL file(s) depend on whether the input        used inline text, or documents.        If input was inline, then each .JSONL file will contain, per line,          a JSON representation of a proto that wraps given in request text          snippet's 'id' (if specified), followed by input text snippet,          and a list of zero or more          AnnotationPayload protos (called annotations), which have          text_extraction detail populated. A single text snippet will be          listed only once with all its annotations, and its annotations will          never be split across files.        If input used documents, then each .JSONL file will contain, per          line, a JSON representation of a proto that wraps given in request          document proto, followed by its OCR-ed representation in the form          of a text snippet, finally followed by a list of zero or more          AnnotationPayload protos (called annotations), which have          text_extraction detail populated and refer, via their indices, to          the OCR-ed text snippet. A single document (and its text snippet)          will be listed only once with all its annotations, and its          annotations will never be split across files.        If prediction for any text snippet failed (partially or completely),        then additional `errors_1.jsonl`, `errors_2.jsonl`,...,        `errors_N.jsonl` files will be created (N depends on total number of        failed predictions). These files will have a JSON representation of a        proto that wraps either the 'id' : '<id_value>' (in case of inline)        or the document proto (in case of document) but here followed by        exactly one[`google.rpc.Status`](https://github.com/googleapis/googleapis/blob/master/google/rpc/status.proto)        containing only `code` and `message`. *  For Tables:        Output depends on whethergcs_destination        orbigquery_destination        is set (either is allowed).        Google Cloud Storage case:          In the created directory files `tables_1.csv`, `tables_2.csv`,...,          `tables_N.csv` will be created, where N may be 1, and depends on          the total number of the successfully predicted rows.          For all CLASSIFICATIONprediction_type-s:            Each .csv file will contain a header, listing all columns'display_name-s            given on input followed by M target column names in the format of'<target_column_specsdisplay_name>_<target            value>_score' where M is the number of distinct target values,            i.e. number of distinct values in the target column of the table            used to train the model. Subsequent lines will contain the            respective values of successfully predicted rows, with the last,            i.e. the target, columns having the corresponding prediction            scores.          For REGRESSION and FORECASTINGprediction_type-s:            Each .csv file will contain a header, listing all columns'            display_name-s            given on input followed by the predicted target column with name            in the format of'predicted_<target_column_specsdisplay_name>'            Subsequent lines will contain the respective values of            successfully predicted rows, with the last, i.e. the target,            column having the predicted target value.            If prediction for any rows failed, then an additional            `errors_1.csv`, `errors_2.csv`,..., `errors_N.csv` will be            created (N depends on total number of failed rows). These files            will have analogous format as `tables_*.csv`, but always with a            single target column having[`google.rpc.Status`](https://github.com/googleapis/googleapis/blob/master/google/rpc/status.proto)            represented as a JSON string, and containing only `code` and            `message`.        BigQuery case:bigquery_destination          pointing to a BigQuery project must be set. In the given project a          new dataset will be created with name          `prediction_<model-display-name>_<timestamp-of-prediction-call>`          where <model-display-name> will be made          BigQuery-dataset-name compatible (e.g. most special characters will          become underscores), and timestamp will be in          YYYY_MM_DDThh_mm_ss_sssZ 'based on ISO-8601' format. In the dataset          two tables will be created, `predictions`, and `errors`.          The `predictions` table's column names will be the input columns'display_name-s          followed by the target column with name in the format of'predicted_<target_column_specsdisplay_name>'          The input feature columns will contain the respective values of          successfully predicted rows, with the target column having an          ARRAY ofAnnotationPayloads,          represented as STRUCT-s, containing          TablesAnnotation.          The `errors` table contains rows for which the prediction has          failed, it has analogous input columns while the target column name          is in the format of'errors_<target_column_specsdisplay_name>',          and as a value has[`google.rpc.Status`](https://github.com/googleapis/googleapis/blob/master/google/rpc/status.proto)          represented as a STRUCT, and containing only `code` and `message`.
#' 
#' @param gcsDestination Required
#' 
#' @return BatchPredictOutputConfig object
#' 
#' @family BatchPredictOutputConfig functions
#' @export
BatchPredictOutputConfig <- function(gcsDestination = NULL) {
    structure(list(gcsDestination = gcsDestination), class = "gar_BatchPredictOutputConfig")
}

#' PredictResponse Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Response message for PredictionService.Predict.
#' 
#' @param PredictResponse.metadata The \link{PredictResponse.metadata} object or list of objects
#' @param payload Prediction result
#' @param preprocessedInput The preprocessed example that AutoML actually makes prediction on
#' @param metadata Additional domain-specific prediction response metadata
#' 
#' @return PredictResponse object
#' 
#' @family PredictResponse functions
#' @export
PredictResponse <- function(PredictResponse.metadata = NULL, payload = NULL, preprocessedInput = NULL, 
    metadata = NULL) {
    structure(list(PredictResponse.metadata = PredictResponse.metadata, payload = payload, 
        preprocessedInput = preprocessedInput, metadata = metadata), class = "gar_PredictResponse")
}

#' PredictResponse.metadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Additional domain-specific prediction response metadata.<h4>AutoML Vision Object Detection</h4>`max_bounding_box_count`: (int64) The maximum number of bounding boxes to return per image.<h4>AutoML Natural Language Sentiment Analysis</h4>`sentiment_score`: (float, deprecated) A value between -1 and 1,  -1 maps to least positive sentiment, while 1 maps to the most positive  one and the higher the score, the more positive the sentiment in the  document is. Yet these values are relative to the training data, so  e.g. if all data was positive then -1 is also positive (though  the least).  `sentiment_score` is not the same as 'score' and 'magnitude'  from Sentiment Analysis in the Natural Language API.
#' 
#' 
#' 
#' @return PredictResponse.metadata object
#' 
#' @family PredictResponse functions
#' @export
PredictResponse.metadata <- function() {
    list()
}

#' Document Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' A structured text document e.g. a PDF.
#' 
#' @param documentText The plain text version of this document
#' @param layout Describes the layout of the document
#' @param pageCount Number of pages in the document
#' @param documentDimensions The dimensions of the page in the document
#' @param inputConfig An input config specifying the content of the document
#' 
#' @return Document object
#' 
#' @family Document functions
#' @export
Document <- function(documentText = NULL, layout = NULL, pageCount = NULL, documentDimensions = NULL, 
    inputConfig = NULL) {
    structure(list(documentText = documentText, layout = layout, pageCount = pageCount, 
        documentDimensions = documentDimensions, inputConfig = inputConfig), class = "gar_Document")
}

#' ListOperationsResponse Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' The response message for Operations.ListOperations.
#' 
#' @param operations A list of operations that matches the specified filter in the request
#' @param nextPageToken The standard List next-page token
#' 
#' @return ListOperationsResponse object
#' 
#' @family ListOperationsResponse functions
#' @export
ListOperationsResponse <- function(operations = NULL, nextPageToken = NULL) {
    structure(list(operations = operations, nextPageToken = nextPageToken), class = "gar_ListOperationsResponse")
}

#' OperationMetadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Metadata used across all long running operations returned by AutoML API.
#' 
#' @param batchPredictDetails Details of BatchPredict operation
#' @param progressPercent Output only
#' @param deployModelDetails Details of a DeployModel operation
#' @param deleteDetails Details of a Delete operation
#' @param exportModelDetails Details of ExportModel operation
#' @param createModelDetails Details of CreateModel operation
#' @param createTime Output only
#' @param exportDataDetails Details of ExportData operation
#' @param updateTime Output only
#' @param createDatasetDetails Details of CreateDataset operation
#' @param undeployModelDetails Details of an UndeployModel operation
#' @param partialFailures Output only
#' @param importDataDetails Details of ImportData operation
#' 
#' @return OperationMetadata object
#' 
#' @family OperationMetadata functions
#' @export
OperationMetadata <- function(batchPredictDetails = NULL, progressPercent = NULL, 
    deployModelDetails = NULL, deleteDetails = NULL, exportModelDetails = NULL, createModelDetails = NULL, 
    createTime = NULL, exportDataDetails = NULL, updateTime = NULL, createDatasetDetails = NULL, 
    undeployModelDetails = NULL, partialFailures = NULL, importDataDetails = NULL) {
    structure(list(batchPredictDetails = batchPredictDetails, progressPercent = progressPercent, 
        deployModelDetails = deployModelDetails, deleteDetails = deleteDetails, exportModelDetails = exportModelDetails, 
        createModelDetails = createModelDetails, createTime = createTime, exportDataDetails = exportDataDetails, 
        updateTime = updateTime, createDatasetDetails = createDatasetDetails, undeployModelDetails = undeployModelDetails, 
        partialFailures = partialFailures, importDataDetails = importDataDetails), 
        class = "gar_OperationMetadata")
}

#' DeployModelOperationMetadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Details of DeployModel operation.
#' 
#' 
#' 
#' @return DeployModelOperationMetadata object
#' 
#' @family DeployModelOperationMetadata functions
#' @export
DeployModelOperationMetadata <- function() {
    list()
}

#' UndeployModelOperationMetadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Details of UndeployModel operation.
#' 
#' 
#' 
#' @return UndeployModelOperationMetadata object
#' 
#' @family UndeployModelOperationMetadata functions
#' @export
UndeployModelOperationMetadata <- function() {
    list()
}

#' ImageObjectDetectionDatasetMetadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Dataset metadata specific to image object detection.
#' 
#' 
#' 
#' @return ImageObjectDetectionDatasetMetadata object
#' 
#' @family ImageObjectDetectionDatasetMetadata functions
#' @export
ImageObjectDetectionDatasetMetadata <- function() {
    list()
}

#' ClassificationEvaluationMetricsConfusionMatrixRow Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Output only. A row in the confusion matrix.
#' 
#' @param exampleCount Output only
#' 
#' @return ClassificationEvaluationMetricsConfusionMatrixRow object
#' 
#' @family ClassificationEvaluationMetricsConfusionMatrixRow functions
#' @export
ClassificationEvaluationMetricsConfusionMatrixRow <- function(exampleCount = NULL) {
    structure(list(exampleCount = exampleCount), class = "gar_ClassificationEvaluationMetricsConfusionMatrixRow")
}

#' BoundingBoxMetricsEntry Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Bounding box matching model metrics for a single intersection-over-unionthreshold and multiple label match confidence thresholds.
#' 
#' @param iouThreshold Output only
#' @param meanAveragePrecision Output only
#' @param confidenceMetricsEntries Output only
#' 
#' @return BoundingBoxMetricsEntry object
#' 
#' @family BoundingBoxMetricsEntry functions
#' @export
BoundingBoxMetricsEntry <- function(iouThreshold = NULL, meanAveragePrecision = NULL, 
    confidenceMetricsEntries = NULL) {
    structure(list(iouThreshold = iouThreshold, meanAveragePrecision = meanAveragePrecision, 
        confidenceMetricsEntries = confidenceMetricsEntries), class = "gar_BoundingBoxMetricsEntry")
}

#' BoundingBoxMetricsEntryConfidenceMetricsEntry Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Metrics for a single confidence threshold.
#' 
#' @param confidenceThreshold Output only
#' @param recall Output only
#' @param f1Score Output only
#' @param precision Output only
#' 
#' @return BoundingBoxMetricsEntryConfidenceMetricsEntry object
#' 
#' @family BoundingBoxMetricsEntryConfidenceMetricsEntry functions
#' @export
BoundingBoxMetricsEntryConfidenceMetricsEntry <- function(confidenceThreshold = NULL, 
    recall = NULL, f1Score = NULL, precision = NULL) {
    structure(list(confidenceThreshold = confidenceThreshold, recall = recall, f1Score = f1Score, 
        precision = precision), class = "gar_BoundingBoxMetricsEntryConfidenceMetricsEntry")
}

#' ImportDataOperationMetadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Details of ImportData operation.
#' 
#' 
#' 
#' @return ImportDataOperationMetadata object
#' 
#' @family ImportDataOperationMetadata functions
#' @export
ImportDataOperationMetadata <- function() {
    list()
}

#' ImageObjectDetectionEvaluationMetrics Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Model evaluation metrics for image object detection problems.Evaluates prediction quality of labeled bounding boxes.
#' 
#' @param boundingBoxMeanAveragePrecision Output only
#' @param evaluatedBoundingBoxCount Output only
#' @param boundingBoxMetricsEntries Output only
#' 
#' @return ImageObjectDetectionEvaluationMetrics object
#' 
#' @family ImageObjectDetectionEvaluationMetrics functions
#' @export
ImageObjectDetectionEvaluationMetrics <- function(boundingBoxMeanAveragePrecision = NULL, 
    evaluatedBoundingBoxCount = NULL, boundingBoxMetricsEntries = NULL) {
    structure(list(boundingBoxMeanAveragePrecision = boundingBoxMeanAveragePrecision, 
        evaluatedBoundingBoxCount = evaluatedBoundingBoxCount, boundingBoxMetricsEntries = boundingBoxMetricsEntries), 
        class = "gar_ImageObjectDetectionEvaluationMetrics")
}

#' CreateModelOperationMetadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Details of CreateModel operation.
#' 
#' 
#' 
#' @return CreateModelOperationMetadata object
#' 
#' @family CreateModelOperationMetadata functions
#' @export
CreateModelOperationMetadata <- function() {
    list()
}

#' BoundingPoly Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' A bounding polygon of a detected object on a plane.On output both vertices and normalized_vertices are provided.The polygon is formed by connecting vertices in the order they are listed.
#' 
#' @param normalizedVertices Output only 
#' 
#' @return BoundingPoly object
#' 
#' @family BoundingPoly functions
#' @export
BoundingPoly <- function(normalizedVertices = NULL) {
    structure(list(normalizedVertices = normalizedVertices), class = "gar_BoundingPoly")
}

#' TextClassificationDatasetMetadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Dataset metadata for classification.
#' 
#' @param classificationType Required
#' 
#' @return TextClassificationDatasetMetadata object
#' 
#' @family TextClassificationDatasetMetadata functions
#' @export
TextClassificationDatasetMetadata <- function(classificationType = NULL) {
    structure(list(classificationType = classificationType), class = "gar_TextClassificationDatasetMetadata")
}

#' TextSentimentDatasetMetadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Dataset metadata for text sentiment.
#' 
#' @param sentimentMax Required
#' 
#' @return TextSentimentDatasetMetadata object
#' 
#' @family TextSentimentDatasetMetadata functions
#' @export
TextSentimentDatasetMetadata <- function(sentimentMax = NULL) {
    structure(list(sentimentMax = sentimentMax), class = "gar_TextSentimentDatasetMetadata")
}

#' ClassificationEvaluationMetrics Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Model evaluation metrics for classification problems.Note: For Video Classification this metrics only describe quality of theVideo Classification predictions of 'segment_classification' type.
#' 
#' @param confidenceMetricsEntry Output only
#' @param auRoc Output only
#' @param annotationSpecId Output only
#' @param logLoss Output only
#' @param auPrc Output only
#' @param confusionMatrix Output only
#' 
#' @return ClassificationEvaluationMetrics object
#' 
#' @family ClassificationEvaluationMetrics functions
#' @export
ClassificationEvaluationMetrics <- function(confidenceMetricsEntry = NULL, auRoc = NULL, 
    annotationSpecId = NULL, logLoss = NULL, auPrc = NULL, confusionMatrix = NULL) {
    structure(list(confidenceMetricsEntry = confidenceMetricsEntry, auRoc = auRoc, 
        annotationSpecId = annotationSpecId, logLoss = logLoss, auPrc = auPrc, confusionMatrix = confusionMatrix), 
        class = "gar_ClassificationEvaluationMetrics")
}

#' BatchPredictResult Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Result of the Batch Predict. This message is returned inresponse of the operation returnedby the PredictionService.BatchPredict.
#' 
#' @param BatchPredictResult.metadata The \link{BatchPredictResult.metadata} object or list of objects
#' @param metadata Additional domain-specific prediction response metadata
#' 
#' @return BatchPredictResult object
#' 
#' @family BatchPredictResult functions
#' @export
BatchPredictResult <- function(BatchPredictResult.metadata = NULL, metadata = NULL) {
    structure(list(BatchPredictResult.metadata = BatchPredictResult.metadata, metadata = metadata), 
        class = "gar_BatchPredictResult")
}

#' BatchPredictResult.metadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Additional domain-specific prediction response metadata.<h4>AutoML Vision Object Detection</h4>`max_bounding_box_count`: (int64) The maximum number of bounding boxes returned per image.<h4>AutoML Video Intelligence Object Tracking</h4>`max_bounding_box_count`: (int64) The maximum number of bounding boxes returned per frame.
#' 
#' 
#' 
#' @return BatchPredictResult.metadata object
#' 
#' @family BatchPredictResult functions
#' @export
BatchPredictResult.metadata <- function() {
    list()
}

#' SetIamPolicyRequest Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Request message for `SetIamPolicy` method.
#' 
#' @param policy REQUIRED: The complete policy to be applied to the `resource`
#' 
#' @return SetIamPolicyRequest object
#' 
#' @family SetIamPolicyRequest functions
#' @export
SetIamPolicyRequest <- function(policy = NULL) {
    structure(list(policy = policy), class = "gar_SetIamPolicyRequest")
}

#' ImageClassificationModelDeploymentMetadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Model deployment metadata specific to Image Classification.
#' 
#' @param nodeCount Input only
#' 
#' @return ImageClassificationModelDeploymentMetadata object
#' 
#' @family ImageClassificationModelDeploymentMetadata functions
#' @export
ImageClassificationModelDeploymentMetadata <- function(nodeCount = NULL) {
    structure(list(nodeCount = nodeCount), class = "gar_ImageClassificationModelDeploymentMetadata")
}

#' ImageClassificationModelMetadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Model metadata for image classification.
#' 
#' @param modelType Optional
#' @param trainBudgetMilliNodeHours The train budget of creating this model, expressed in milli node
#' @param stopReason Output only
#' @param nodeQps Output only
#' @param trainCostMilliNodeHours Output only
#' @param baseModelId Optional
#' @param nodeCount Output only
#' 
#' @return ImageClassificationModelMetadata object
#' 
#' @family ImageClassificationModelMetadata functions
#' @export
ImageClassificationModelMetadata <- function(modelType = NULL, trainBudgetMilliNodeHours = NULL, 
    stopReason = NULL, nodeQps = NULL, trainCostMilliNodeHours = NULL, baseModelId = NULL, 
    nodeCount = NULL) {
    structure(list(modelType = modelType, trainBudgetMilliNodeHours = trainBudgetMilliNodeHours, 
        stopReason = stopReason, nodeQps = nodeQps, trainCostMilliNodeHours = trainCostMilliNodeHours, 
        baseModelId = baseModelId, nodeCount = nodeCount), class = "gar_ImageClassificationModelMetadata")
}

#' ExportModelOutputInfo Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Further describes the output of model export.SupplementsModelExportOutputConfig.
#' 
#' @param gcsOutputDirectory The full path of the Google Cloud Storage directory created, into which
#' 
#' @return ExportModelOutputInfo object
#' 
#' @family ExportModelOutputInfo functions
#' @export
ExportModelOutputInfo <- function(gcsOutputDirectory = NULL) {
    structure(list(gcsOutputDirectory = gcsOutputDirectory), class = "gar_ExportModelOutputInfo")
}

#' ExportDataRequest Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Request message for AutoMl.ExportData.
#' 
#' @param outputConfig Required
#' 
#' @return ExportDataRequest object
#' 
#' @family ExportDataRequest functions
#' @export
ExportDataRequest <- function(outputConfig = NULL) {
    structure(list(outputConfig = outputConfig), class = "gar_ExportDataRequest")
}

#' Location Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' A resource that represents Google Cloud Platform location.
#' 
#' @param Location.metadata The \link{Location.metadata} object or list of objects
#' @param Location.labels The \link{Location.labels} object or list of objects
#' @param name Resource name for the location, which may vary between implementations
#' @param locationId The canonical id for this location
#' @param displayName The friendly name for this location, typically a nearby city name
#' @param metadata Service-specific metadata
#' @param labels Cross-service attributes for the location
#' 
#' @return Location object
#' 
#' @family Location functions
#' @export
Location <- function(Location.metadata = NULL, Location.labels = NULL, name = NULL, 
    locationId = NULL, displayName = NULL, metadata = NULL, labels = NULL) {
    structure(list(Location.metadata = Location.metadata, Location.labels = Location.labels, 
        name = name, locationId = locationId, displayName = displayName, metadata = metadata, 
        labels = labels), class = "gar_Location")
}

#' Location.metadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Service-specific metadata. For example the available capacity at the givenlocation.
#' 
#' 
#' 
#' @return Location.metadata object
#' 
#' @family Location functions
#' @export
Location.metadata <- function() {
    list()
}

#' Location.labels Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Cross-service attributes for the location. For example    {'cloud.googleapis.com/region': 'us-east1'}
#' 
#' 
#' 
#' @return Location.labels object
#' 
#' @family Location functions
#' @export
Location.labels <- function() {
    list()
}

#' ClassificationAnnotation Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Contains annotation details specific to classification.
#' 
#' @param score Output only
#' 
#' @return ClassificationAnnotation object
#' 
#' @family ClassificationAnnotation functions
#' @export
ClassificationAnnotation <- function(score = NULL) {
    structure(list(score = score), class = "gar_ClassificationAnnotation")
}

#' ImageObjectDetectionModelMetadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Model metadata specific to image object detection.
#' 
#' @param trainCostMilliNodeHours Output only
#' @param nodeQps Output only
#' @param nodeCount Output only
#' @param modelType Optional
#' @param trainBudgetMilliNodeHours The train budget of creating this model, expressed in milli node
#' @param stopReason Output only
#' 
#' @return ImageObjectDetectionModelMetadata object
#' 
#' @family ImageObjectDetectionModelMetadata functions
#' @export
ImageObjectDetectionModelMetadata <- function(trainCostMilliNodeHours = NULL, nodeQps = NULL, 
    nodeCount = NULL, modelType = NULL, trainBudgetMilliNodeHours = NULL, stopReason = NULL) {
    structure(list(trainCostMilliNodeHours = trainCostMilliNodeHours, nodeQps = nodeQps, 
        nodeCount = nodeCount, modelType = modelType, trainBudgetMilliNodeHours = trainBudgetMilliNodeHours, 
        stopReason = stopReason), class = "gar_ImageObjectDetectionModelMetadata")
}

#' TextExtractionDatasetMetadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Dataset metadata that is specific to text extraction
#' 
#' 
#' 
#' @return TextExtractionDatasetMetadata object
#' 
#' @family TextExtractionDatasetMetadata functions
#' @export
TextExtractionDatasetMetadata <- function() {
    list()
}

#' ExportModelOperationMetadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Details of ExportModel operation.
#' 
#' @param outputInfo Output only
#' 
#' @return ExportModelOperationMetadata object
#' 
#' @family ExportModelOperationMetadata functions
#' @export
ExportModelOperationMetadata <- function(outputInfo = NULL) {
    structure(list(outputInfo = outputInfo), class = "gar_ExportModelOperationMetadata")
}

#' TranslationModelMetadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Model metadata that is specific to translation.
#' 
#' @param sourceLanguageCode Output only
#' @param baseModel The resource name of the model to use as a baseline to train the custom
#' @param targetLanguageCode Output only
#' 
#' @return TranslationModelMetadata object
#' 
#' @family TranslationModelMetadata functions
#' @export
TranslationModelMetadata <- function(sourceLanguageCode = NULL, baseModel = NULL, 
    targetLanguageCode = NULL) {
    structure(list(sourceLanguageCode = sourceLanguageCode, baseModel = baseModel, 
        targetLanguageCode = targetLanguageCode), class = "gar_TranslationModelMetadata")
}

#' ImageObjectDetectionAnnotation Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Annotation details for image object detection.
#' 
#' @param boundingBox Output only
#' @param score Output only
#' 
#' @return ImageObjectDetectionAnnotation object
#' 
#' @family ImageObjectDetectionAnnotation functions
#' @export
ImageObjectDetectionAnnotation <- function(boundingBox = NULL, score = NULL) {
    structure(list(boundingBox = boundingBox, score = score), class = "gar_ImageObjectDetectionAnnotation")
}

#' ListDatasetsResponse Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Response message for AutoMl.ListDatasets.
#' 
#' @param datasets The datasets read
#' @param nextPageToken A token to retrieve next page of results
#' 
#' @return ListDatasetsResponse object
#' 
#' @family ListDatasetsResponse functions
#' @export
ListDatasetsResponse <- function(datasets = NULL, nextPageToken = NULL) {
    structure(list(datasets = datasets, nextPageToken = nextPageToken), class = "gar_ListDatasetsResponse")
}

#' TranslationDatasetMetadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Dataset metadata that is specific to translation.
#' 
#' @param targetLanguageCode Required
#' @param sourceLanguageCode Required
#' 
#' @return TranslationDatasetMetadata object
#' 
#' @family TranslationDatasetMetadata functions
#' @export
TranslationDatasetMetadata <- function(targetLanguageCode = NULL, sourceLanguageCode = NULL) {
    structure(list(targetLanguageCode = targetLanguageCode, sourceLanguageCode = sourceLanguageCode), 
        class = "gar_TranslationDatasetMetadata")
}

#' TestIamPermissionsRequest Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Request message for `TestIamPermissions` method.
#' 
#' @param permissions The set of permissions to check for the `resource`
#' 
#' @return TestIamPermissionsRequest object
#' 
#' @family TestIamPermissionsRequest functions
#' @export
TestIamPermissionsRequest <- function(permissions = NULL) {
    structure(list(permissions = permissions), class = "gar_TestIamPermissionsRequest")
}

#' Image Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' A representation of an image.Only images up to 30MB in size are supported.
#' 
#' @param thumbnailUri Output only
#' @param imageBytes Image content represented as a stream of bytes
#' 
#' @return Image object
#' 
#' @family Image functions
#' @export
Image <- function(thumbnailUri = NULL, imageBytes = NULL) {
    structure(list(thumbnailUri = thumbnailUri, imageBytes = imageBytes), class = "gar_Image")
}

#' GcsSource Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' The Google Cloud Storage location for the input content.
#' 
#' @param inputUris Required
#' 
#' @return GcsSource object
#' 
#' @family GcsSource functions
#' @export
GcsSource <- function(inputUris = NULL) {
    structure(list(inputUris = inputUris), class = "gar_GcsSource")
}

#' InputConfig Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Input configuration for AutoMl.ImportData action.The format of input depends on dataset_metadata the Dataset into whichthe import is happening has. As input source thegcs_sourceis expected, unless specified otherwise. Additionally any input .CSV fileby itself must be 100MB or smaller, unless specified otherwise.If an 'example' file (that is, image, video etc.) with identical content(even if it had different `GCS_FILE_PATH`) is mentioned multiple times, thenits label, bounding boxes etc. are appended. The same file should be alwaysprovided with the same `ML_USE` and `GCS_FILE_PATH`, if it is not, thenthese values are nondeterministically selected from the given ones.The formats are represented in EBNF with commas being literal and withnon-terminal symbols defined near the end of this comment. The formats are:<h4>AutoML Vision</h4><div class='ds-selector-tabs'><section><h5>Classification</h5>See [Preparing your trainingdata](https://cloud.google.com/vision/automl/docs/prepare) for moreinformation.CSV file(s) with each line in format:    ML_USE,GCS_FILE_PATH,LABEL,LABEL,...*   `ML_USE` - Identifies the data set that the current row (file) appliesto.    This value can be one of the following:    * `TRAIN` - Rows in this file are used to train the model.    * `TEST` - Rows in this file are used to test the model during training.    * `UNASSIGNED` - Rows in this file are not categorized. They are       Automatically divided into train and test data. 80% for training and       20% for testing.*   `GCS_FILE_PATH` - The Google Cloud Storage location of an image of up to     30MB in size. Supported extensions: .JPEG, .GIF, .PNG, .WEBP, .BMP,     .TIFF, .ICO.*   `LABEL` - A label that identifies the object in the image.For the `MULTICLASS` classification type, at most one `LABEL` is allowedper image. If an image has not yet been labeled, then it should bementioned just once with no `LABEL`.Some sample rows:    TRAIN,gs://folder/image1.jpg,daisy    TEST,gs://folder/image2.jpg,dandelion,tulip,rose    UNASSIGNED,gs://folder/image3.jpg,daisy    UNASSIGNED,gs://folder/image4.jpg</section><section><h5>Object Detection</h5>See [Preparing your trainingdata](https://cloud.google.com/vision/automl/object-detection/docs/prepare)for more information.A CSV file(s) with each line in format:    ML_USE,GCS_FILE_PATH,[LABEL],(BOUNDING_BOX | ,,,,,,,)*   `ML_USE` - Identifies the data set that the current row (file) appliesto.    This value can be one of the following:    * `TRAIN` - Rows in this file are used to train the model.    * `TEST` - Rows in this file are used to test the model during training.    * `UNASSIGNED` - Rows in this file are not categorized. They are       Automatically divided into train and test data. 80% for training and       20% for testing.*  `GCS_FILE_PATH` - The Google Cloud Storage location of an image of up to    30MB in size. Supported extensions: .JPEG, .GIF, .PNG. Each image    is assumed to be exhaustively labeled.*  `LABEL` - A label that identifies the object in the image specified by the   `BOUNDING_BOX`.*  `BOUNDING BOX` - The vertices of an object in the example image.   The minimum allowed `BOUNDING_BOX` edge length is 0.01, and no more than   500 `BOUNDING_BOX` instances per image are allowed (one `BOUNDING_BOX`   per line). If an image has no looked for objects then it should be   mentioned just once with no LABEL and the ',,,,,,,' in place of the  `BOUNDING_BOX`.**Four sample rows:**    TRAIN,gs://folder/image1.png,car,0.1,0.1,,,0.3,0.3,,    TRAIN,gs://folder/image1.png,bike,.7,.6,,,.8,.9,,    UNASSIGNED,gs://folder/im2.png,car,0.1,0.1,0.2,0.1,0.2,0.3,0.1,0.3    TEST,gs://folder/im3.png,,,,,,,,,  </section></div><h4>AutoML Video Intelligence</h4><div class='ds-selector-tabs'><section><h5>Classification</h5>See [Preparing your trainingdata](https://cloud.google.com/video-intelligence/automl/docs/prepare) formore information.CSV file(s) with each line in format:    ML_USE,GCS_FILE_PATHFor `ML_USE`, do not use `VALIDATE`.`GCS_FILE_PATH` is the path to another .csv file that describes trainingexample for a given `ML_USE`, using the following row format:    GCS_FILE_PATH,(LABEL,TIME_SEGMENT_START,TIME_SEGMENT_END | ,,)Here `GCS_FILE_PATH` leads to a video of up to 50GB in size and upto 3h duration. Supported extensions: .MOV, .MPEG4, .MP4, .AVI.`TIME_SEGMENT_START` and `TIME_SEGMENT_END` must be within thelength of the video, and the end time must be after the start time. Anysegment of a video which has one or more labels on it, is considered ahard negative for all other labels. Any segment with no labels onit is considered to be unknown. If a whole video is unknown, thenit should be mentioned just once with ',,' in place of `LABEL,TIME_SEGMENT_START,TIME_SEGMENT_END`.Sample top level CSV file:    TRAIN,gs://folder/train_videos.csv    TEST,gs://folder/test_videos.csv    UNASSIGNED,gs://folder/other_videos.csvSample rows of a CSV file for a particular ML_USE:    gs://folder/video1.avi,car,120,180.000021    gs://folder/video1.avi,bike,150,180.000021    gs://folder/vid2.avi,car,0,60.5    gs://folder/vid3.avi,,,</section><section><h5>Object Tracking</h5>See [Preparing your trainingdata](/video-intelligence/automl/object-tracking/docs/prepare) for moreinformation.CSV file(s) with each line in format:    ML_USE,GCS_FILE_PATHFor `ML_USE`, do not use `VALIDATE`.`GCS_FILE_PATH` is the path to another .csv file that describes trainingexample for a given `ML_USE`, using the following row format:    GCS_FILE_PATH,LABEL,[INSTANCE_ID],TIMESTAMP,BOUNDING_BOXor    GCS_FILE_PATH,,,,,,,,,,Here `GCS_FILE_PATH` leads to a video of up to 50GB in size and upto 3h duration. Supported extensions: .MOV, .MPEG4, .MP4, .AVI.Providing `INSTANCE_ID`s can help to obtain a better model. Whena specific labeled entity leaves the video frame, and shows upafterwards it is not required, albeit preferable, that the same`INSTANCE_ID` is given to it.`TIMESTAMP` must be within the length of the video, the`BOUNDING_BOX` is assumed to be drawn on the closest video's frameto the `TIMESTAMP`. Any mentioned by the `TIMESTAMP` frame is expectedto be exhaustively labeled and no more than 500 `BOUNDING_BOX`-es perframe are allowed. If a whole video is unknown, then it should bementioned just once with ',,,,,,,,,,' in place of `LABEL,[INSTANCE_ID],TIMESTAMP,BOUNDING_BOX`.Sample top level CSV file:     TRAIN,gs://folder/train_videos.csv     TEST,gs://folder/test_videos.csv     UNASSIGNED,gs://folder/other_videos.csvSeven sample rows of a CSV file for a particular ML_USE:     gs://folder/video1.avi,car,1,12.10,0.8,0.8,0.9,0.8,0.9,0.9,0.8,0.9     gs://folder/video1.avi,car,1,12.90,0.4,0.8,0.5,0.8,0.5,0.9,0.4,0.9     gs://folder/video1.avi,car,2,12.10,.4,.2,.5,.2,.5,.3,.4,.3     gs://folder/video1.avi,car,2,12.90,.8,.2,,,.9,.3,,     gs://folder/video1.avi,bike,,12.50,.45,.45,,,.55,.55,,     gs://folder/video2.avi,car,1,0,.1,.9,,,.9,.1,,     gs://folder/video2.avi,,,,,,,,,,,  </section></div><h4>AutoML Natural Language</h4><div class='ds-selector-tabs'><section><h5>Entity Extraction</h5>See [Preparing your trainingdata](/natural-language/automl/entity-analysis/docs/prepare) for moreinformation.One or more CSV file(s) with each line in the following format:    ML_USE,GCS_FILE_PATH*   `ML_USE` - Identifies the data set that the current row (file) appliesto.    This value can be one of the following:    * `TRAIN` - Rows in this file are used to train the model.    * `TEST` - Rows in this file are used to test the model during training.    * `UNASSIGNED` - Rows in this file are not categorized. They are       Automatically divided into train and test data. 80% for training and       20% for testing..*   `GCS_FILE_PATH` - a Identifies JSON Lines (.JSONL) file stored in     Google Cloud Storage that contains in-line text in-line as documents     for model training.After the training data set has been determined from the `TRAIN` and`UNASSIGNED` CSV files, the training data is divided into train andvalidation data sets. 70% for training and 30% for validation.For example:    TRAIN,gs://folder/file1.jsonl    VALIDATE,gs://folder/file2.jsonl    TEST,gs://folder/file3.jsonl**In-line JSONL files**In-line .JSONL files contain, per line, a JSON document that wraps a`text_snippet` field followed byone or more `annotations`fields, which have `display_name` and `text_extraction` fields to describethe entity from the text snippet. Multiple JSON documents can be separatedusing line breaks (\n).The supplied text must be annotated exhaustively. For example, if youinclude the text 'horse', but do not label it as 'animal',then 'horse' is assumed to not be an 'animal'.Any given text snippet content must have 30,000 characters orless, and also be UTF-8 NFC encoded. ASCII is accepted as it isUTF-8 NFC encoded.For example:    {      'text_snippet': {        'content': 'dog car cat'      },      'annotations': [         {           'display_name': 'animal',           'text_extraction': {             'text_segment': {'start_offset': 0, 'end_offset': 2}          }         },         {          'display_name': 'vehicle',           'text_extraction': {             'text_segment': {'start_offset': 4, 'end_offset': 6}           }         },         {           'display_name': 'animal',           'text_extraction': {             'text_segment': {'start_offset': 8, 'end_offset': 10}           }         }     ]    }\n    {       'text_snippet': {         'content': 'This dog is good.'       },       'annotations': [          {            'display_name': 'animal',            'text_extraction': {              'text_segment': {'start_offset': 5, 'end_offset': 7}            }          }       ]    }**JSONL files that reference documents**.JSONL files contain, per line, a JSON document that wraps a`input_config` that contains the path to a source PDF document.Multiple JSON documents can be separated using line breaks (\n).For example:    {      'document': {        'input_config': {          'gcs_source': { 'input_uris': [ 'gs://folder/document1.pdf' ]          }        }      }    }\n    {      'document': {        'input_config': {          'gcs_source': { 'input_uris': [ 'gs://folder/document2.pdf' ]          }        }      }    }**In-line JSONL files with PDF layout information****Note:** You can only annotate PDF files using the UI. The format describedbelow applies to annotated PDF files exported using the UI or `exportData`.In-line .JSONL files for PDF documents contain, per line, a JSON documentthat wraps a `document` field that provides the textual content of the PDFdocument and the layout information.For example:    {      'document': {              'document_text': {                'content': 'dog car cat'              }              'layout': [                {                  'text_segment': {                    'start_offset': 0,                    'end_offset': 11,                   },                   'page_number': 1,                   'bounding_poly': {                      'normalized_vertices': [                        {'x': 0.1, 'y': 0.1},                        {'x': 0.1, 'y': 0.3},                        {'x': 0.3, 'y': 0.3},                        {'x': 0.3, 'y': 0.1},                      ],                    },                    'text_segment_type': TOKEN,                }              ],              'document_dimensions': {                'width': 8.27,                'height': 11.69,                'unit': INCH,              }              'page_count': 3,            },            'annotations': [              {                'display_name': 'animal',                'text_extraction': {                  'text_segment': {'start_offset': 0, 'end_offset': 3}                }              },              {                'display_name': 'vehicle',                'text_extraction': {                  'text_segment': {'start_offset': 4, 'end_offset': 7}                }              },              {                'display_name': 'animal',                'text_extraction': {                  'text_segment': {'start_offset': 8, 'end_offset': 11}                }              },            ],</section><section><h5>Classification</h5>See [Preparing your trainingdata](https://cloud.google.com/natural-language/automl/docs/prepare) for moreinformation.One or more CSV file(s) with each line in the following format:    ML_USE,(TEXT_SNIPPET | GCS_FILE_PATH),LABEL,LABEL,...*   `ML_USE` - Identifies the data set that the current row (file) appliesto.    This value can be one of the following:    * `TRAIN` - Rows in this file are used to train the model.    * `TEST` - Rows in this file are used to test the model during training.    * `UNASSIGNED` - Rows in this file are not categorized. They are       Automatically divided into train and test data. 80% for training and       20% for testing.*   `TEXT_SNIPPET` and `GCS_FILE_PATH` are distinguished by a pattern. If    the column content is a valid Google Cloud Storage file path, that is,    prefixed by 'gs://', it is treated as a `GCS_FILE_PATH`. Otherwise, if    the content is enclosed in double quotes (''), it is treated as a    `TEXT_SNIPPET`. For `GCS_FILE_PATH`, the path must lead to a    file with supported extension and UTF-8 encoding, for example,    'gs://folder/content.txt' AutoML imports the file content    as a text snippet. For `TEXT_SNIPPET`, AutoML imports the column content    excluding quotes. In both cases, size of the content must be 10MB or    less in size. For zip files, the size of each file inside the zip must be    10MB or less in size.    For the `MULTICLASS` classification type, at most one `LABEL` is allowed.    The `ML_USE` and `LABEL` columns are optional.    Supported file extensions: .TXT, .PDF, .ZIPA maximum of 100 unique labels are allowed per CSV row.Sample rows:    TRAIN,'They have bad food and very rude',RudeService,BadFood    gs://folder/content.txt,SlowService    TEST,gs://folder/document.pdf    VALIDATE,gs://folder/text_files.zip,BadFood</section><section><h5>Sentiment Analysis</h5>See [Preparing your trainingdata](https://cloud.google.com/natural-language/automl/docs/prepare) for moreinformation.CSV file(s) with each line in format:    ML_USE,(TEXT_SNIPPET | GCS_FILE_PATH),SENTIMENT*   `ML_USE` - Identifies the data set that the current row (file) appliesto.    This value can be one of the following:    * `TRAIN` - Rows in this file are used to train the model.    * `TEST` - Rows in this file are used to test the model during training.    * `UNASSIGNED` - Rows in this file are not categorized. They are       Automatically divided into train and test data. 80% for training and       20% for testing.*   `TEXT_SNIPPET` and `GCS_FILE_PATH` are distinguished by a pattern. If    the column content is a valid  Google Cloud Storage file path, that is,    prefixed by 'gs://', it is treated as a `GCS_FILE_PATH`. Otherwise, if    the content is enclosed in double quotes (''), it is treated as a    `TEXT_SNIPPET`. For `GCS_FILE_PATH`, the path must lead to a    file with supported extension and UTF-8 encoding, for example,    'gs://folder/content.txt' AutoML imports the file content    as a text snippet. For `TEXT_SNIPPET`, AutoML imports the column content    excluding quotes. In both cases, size of the content must be 128kB or    less in size. For zip files, the size of each file inside the zip must be    128kB or less in size.    The `ML_USE` and `SENTIMENT` columns are optional.    Supported file extensions: .TXT, .PDF, .ZIP*  `SENTIMENT` - An integer between 0 and    Dataset.text_sentiment_dataset_metadata.sentiment_max    (inclusive). Describes the ordinal of the sentiment - higher    value means a more positive sentiment. All the values are    completely relative, i.e. neither 0 needs to mean a negative or    neutral sentiment nor sentiment_max needs to mean a positive one -    it is just required that 0 is the least positive sentiment    in the data, and sentiment_max is the  most positive one.    The SENTIMENT shouldn't be confused with 'score' or 'magnitude'    from the previous Natural Language Sentiment Analysis API.    All SENTIMENT values between 0 and sentiment_max must be    represented in the imported data. On prediction the same 0 to    sentiment_max range will be used. The difference between    neighboring sentiment values needs not to be uniform, e.g. 1 and    2 may be similar whereas the difference between 2 and 3 may be    large.Sample rows:    TRAIN,'@freewrytin this is way too good for your product',2    gs://folder/content.txt,3    TEST,gs://folder/document.pdf    VALIDATE,gs://folder/text_files.zip,2  </section></div><h4>AutoML Tables</h4><div class='ui-datasection-main'><sectionclass='selected'>See [Preparing your trainingdata](https://cloud.google.com/automl-tables/docs/prepare) for moreinformation.You can use eithergcs_source orbigquery_source.All input is concatenated into asingleprimary_table_spec_id**For gcs_source:**CSV file(s), where the first row of the first file is the header,containing unique column names. If the first row of a subsequentfile is the same as the header, then it is also treated as aheader. All other rows contain values for the correspondingcolumns.Each .CSV file by itself must be 10GB or smaller, and their totalsize must be 100GB or smaller.First three sample rows of a CSV file:<pre>'Id','First Name','Last Name','Dob','Addresses''1','John','Doe','1968-01-22','[{'status':'current','address':'123_First_Avenue','city':'Seattle','state':'WA','zip':'11111','numberOfYears':'1'},{'status':'previous','address':'456_Main_Street','city':'Portland','state':'OR','zip':'22222','numberOfYears':'5'}]''2','Jane','Doe','1980-10-16','[{'status':'current','address':'789_Any_Avenue','city':'Albany','state':'NY','zip':'33333','numberOfYears':'2'},{'status':'previous','address':'321_Main_Street','city':'Hoboken','state':'NJ','zip':'44444','numberOfYears':'3'}]}</pre>**For bigquery_source:**An URI of a BigQuery table. The user data size of the BigQuerytable must be 100GB or smaller.An imported table must have between 2 and 1,000 columns, inclusive,and between 1000 and 100,000,000 rows, inclusive. There are at most 5import data running in parallel.  </section></div>**Input field definitions:**`ML_USE`: ('TRAIN' | 'VALIDATE' | 'TEST' | 'UNASSIGNED')  Describes how the given example (file) should be used for model  training. 'UNASSIGNED' can be used when user has no preference.`GCS_FILE_PATH`: The path to a file on Google Cloud Storage. For example,  'gs://folder/image1.png'.`LABEL`: A display name of an object on an image, video etc., e.g. 'dog'.  Must be up to 32 characters long and can consist only of ASCII  Latin letters A-Z and a-z, underscores(_), and ASCII digits 0-9.  For each label an AnnotationSpec is created which display_name  becomes the label; AnnotationSpecs are given back in predictions.`INSTANCE_ID`: A positive integer that identifies a specific instance of a  labeled entity on an example. Used e.g. to track two cars on  a video while being able to tell apart which one is which.`BOUNDING_BOX`: (`VERTEX,VERTEX,VERTEX,VERTEX` | `VERTEX,,,VERTEX,,`)  A rectangle parallel to the frame of the example (image,  video). If 4 vertices are given they are connected by edges  in the order provided, if 2 are given they are recognized  as diagonally opposite vertices of the rectangle.`VERTEX`: (`COORDINATE,COORDINATE`)  First coordinate is horizontal (x), the second is vertical (y).`COORDINATE`: A float in 0 to 1 range, relative to total length of  image or video in given dimension. For fractions the  leading non-decimal 0 can be omitted (i.e. 0.3 = .3).  Point 0,0 is in top left.`TIME_SEGMENT_START`: (`TIME_OFFSET`)  Expresses a beginning, inclusive, of a time segment  within an example that has a time dimension  (e.g. video).`TIME_SEGMENT_END`: (`TIME_OFFSET`)  Expresses an end, exclusive, of a time segment within  n example that has a time dimension (e.g. video).`TIME_OFFSET`: A number of seconds as measured from the start of an  example (e.g. video). Fractions are allowed, up to a  microsecond precision. 'inf' is allowed, and it means the end  of the example.`TEXT_SNIPPET`: The content of a text snippet, UTF-8 encoded, enclosed within  double quotes ('').`DOCUMENT`: A field that provides the textual content with document and the layout  information. **Errors:** If any of the provided CSV files can't be parsed or if more than certain percent of CSV rows cannot be processed then the operation fails and nothing is imported. Regardless of overall success or failure the per-row failures, up to a certain count cap, is listed in Operation.metadata.partial_failures. 
#' 
#' @param InputConfig.params The \link{InputConfig.params} object or list of objects
#' @param gcsSource The Google Cloud Storage location for the input content
#' @param params Additional domain-specific parameters describing the semantic of the
#' 
#' @return InputConfig object
#' 
#' @family InputConfig functions
#' @export
InputConfig <- function(InputConfig.params = NULL, gcsSource = NULL, params = NULL) {
    structure(list(InputConfig.params = InputConfig.params, gcsSource = gcsSource, 
        params = params), class = "gar_InputConfig")
}

#' InputConfig.params Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Additional domain-specific parameters describing the semantic of theimported data, any string must be up to 25000characters long.<h4>AutoML Tables</h4>`schema_inference_version`: (integer) This value must be supplied.  The version of the  algorithm to use for the initial inference of the  column data types of the imported table. Allowed values: '1'.
#' 
#' 
#' 
#' @return InputConfig.params object
#' 
#' @family InputConfig functions
#' @export
InputConfig.params <- function() {
    list()
}

#' BatchPredictOutputInfo Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Further describes this batch predict's output.SupplementsBatchPredictOutputConfig.
#' 
#' @param gcsOutputDirectory The full path of the Google Cloud Storage directory created, into which
#' 
#' @return BatchPredictOutputInfo object
#' 
#' @family BatchPredictOutputInfo functions
#' @export
BatchPredictOutputInfo <- function(gcsOutputDirectory = NULL) {
    structure(list(gcsOutputDirectory = gcsOutputDirectory), class = "gar_BatchPredictOutputInfo")
}

#' BatchPredictRequest Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Request message for PredictionService.BatchPredict.
#' 
#' @param BatchPredictRequest.params The \link{BatchPredictRequest.params} object or list of objects
#' @param outputConfig Required
#' @param params Additional domain-specific parameters for the predictions, any string must
#' @param inputConfig Required
#' 
#' @return BatchPredictRequest object
#' 
#' @family BatchPredictRequest functions
#' @export
BatchPredictRequest <- function(BatchPredictRequest.params = NULL, outputConfig = NULL, 
    params = NULL, inputConfig = NULL) {
    structure(list(BatchPredictRequest.params = BatchPredictRequest.params, outputConfig = outputConfig, 
        params = params, inputConfig = inputConfig), class = "gar_BatchPredictRequest")
}

#' BatchPredictRequest.params Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Additional domain-specific parameters for the predictions, any string mustbe up to 25000 characters long.<h4>AutoML Natural Language Classification</h4>`score_threshold`: (float) A value from 0.0 to 1.0. When the model  makes predictions for a text snippet, it will only produce results  that have at least this confidence score. The default is 0.5.<h4>AutoML Vision Classification</h4>`score_threshold`: (float) A value from 0.0 to 1.0. When the model  makes predictions for an image, it will only produce results that  have at least this confidence score. The default is 0.5.<h4>AutoML Vision Object Detection</h4>`score_threshold`: (float) When Model detects objects on the image,  it will only produce bounding boxes which have at least this  confidence score. Value in 0 to 1 range, default is 0.5.`max_bounding_box_count`: (int64) The maximum number of bounding  boxes returned per image. The default is 100, the  number of bounding boxes returned might be limited by the server.<h4>AutoML Video Intelligence Classification</h4>`score_threshold`: (float) A value from 0.0 to 1.0. When the model  makes predictions for a video, it will only produce results that  have at least this confidence score. The default is 0.5.`segment_classification`: (boolean) Set to true to request  segment-level classification. AutoML Video Intelligence returns  labels and their confidence scores for the entire segment of the  video that user specified in the request configuration.  The default is true.`shot_classification`: (boolean) Set to true to request shot-level  classification. AutoML Video Intelligence determines the boundaries  for each camera shot in the entire segment of the video that user  specified in the request configuration. AutoML Video Intelligence  then returns labels and their confidence scores for each detected  shot, along with the start and end time of the shot.  The default is false.  WARNING: Model evaluation is not done for this classification type,  the quality of it depends on training data, but there are no metrics  provided to describe that quality.`1s_interval_classification`: (boolean) Set to true to request  classification for a video at one-second intervals. AutoML Video  Intelligence returns labels and their confidence scores for each  second of the entire segment of the video that user specified in the  request configuration. The default is false.  WARNING: Model evaluation is not done for this classification  type, the quality of it depends on training data, but there are no  metrics provided to describe that quality.<h4>AutoML Video Intelligence Object Tracking</h4>`score_threshold`: (float) When Model detects objects on video frames,  it will only produce bounding boxes which have at least this  confidence score. Value in 0 to 1 range, default is 0.5.`max_bounding_box_count`: (int64) The maximum number of bounding  boxes returned per image. The default is 100, the  number of bounding boxes returned might be limited by the server.`min_bounding_box_size`: (float) Only bounding boxes with shortest edge  at least that long as a relative value of video frame size are  returned. Value in 0 to 1 range. Default is 0.  
#' 
#' 
#' 
#' @return BatchPredictRequest.params object
#' 
#' @family BatchPredictRequest functions
#' @export
BatchPredictRequest.params <- function() {
    list()
}


#' TextExtractionModelMetadata Object
#' 
#' @details 
#' Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
#' Model metadata that is specific to text extraction.
#' 
#' 
#' 
#' @return TextExtractionModelMetadata object
#' 
#' @family TextExtractionModelMetadata functions
#' @export


TextExtractionModelMetadata <- function() {
    
    
    list()
    
}

